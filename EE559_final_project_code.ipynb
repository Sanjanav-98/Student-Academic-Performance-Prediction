{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "junCmOTVd8X8"
   },
   "source": [
    "# **EE 559 PROJECT**\n",
    "\n",
    "#### **DATASET** : Student Performance Dataset\n",
    "\n",
    "#### **TEAM MEMBERS** :\n",
    "\n",
    "1. Sanjana Vasudeva (USC ID : 7071819723)\n",
    "2. Vaishnavi Channakeshava (USC ID : 9673718359)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_67XDHFZVv7M"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import mutual_info_regression,mutual_info_classif\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn import  ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DeVGwpIJV2UX"
   },
   "outputs": [],
   "source": [
    "class pre_processing:\n",
    "  def __init__(self, filename,mission = None):\n",
    "    self.filename = filename\n",
    "    self.mission = mission\n",
    "  \n",
    "  def preprocess(self, grade):\n",
    "    data = pd.read_csv(self.filename)\n",
    "    data_binary = data.copy()\n",
    "    data_binary.drop(['Mjob', 'Fjob', 'reason', 'guardian'],inplace=True, axis=1)\n",
    "\n",
    "    data_binary_copy = data_binary.copy()\n",
    "    \n",
    "    self.y_data = data_binary_copy[grade]\n",
    "    data_binary_copy.drop([grade], inplace=True, axis=1)\n",
    "\n",
    "    if (self.mission == '3'):\n",
    "      print(\"mission 3\")\n",
    "      pass\n",
    "    else:\n",
    "      data_binary_copy.drop(['G2'], inplace=True, axis=1)\n",
    "      if grade == 'G1':\n",
    "          data_binary_copy.drop(['G3'], inplace=True, axis=1)\n",
    "      else:\n",
    "          data_binary_copy.drop(['G1'], inplace=True, axis=1)\n",
    "    \n",
    "    self.x_data = data_binary_copy\n",
    "\n",
    "    return self.x_data,self.y_data\n",
    "\n",
    "  \n",
    "  def one_hot_encode(self):\n",
    "    categorical_cols = ['school', 'sex', 'address', 'famsize', 'Pstatus', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic'] \n",
    "\n",
    "    data_categorical = self.x_data[categorical_cols]\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    data_one_hot = data_categorical.apply(le.fit_transform)\n",
    "    data_other_cols = self.x_data.drop(columns=categorical_cols)\n",
    "    self.x_enc = pd.concat([data_one_hot, data_other_cols], axis=1)\n",
    "    return self.x_enc\n",
    "\n",
    "\n",
    "  def create_class(self):\n",
    "    g1_classifier = []\n",
    "    for i in self.y_data:\n",
    "      if i<=20 and i>=16:\n",
    "        g1_classifier.append(1)\n",
    "      elif i<=15 and i>=14:\n",
    "        g1_classifier.append(2)\n",
    "      elif i<=13 and i>=12:\n",
    "        g1_classifier.append(3)\n",
    "      elif i<=11 and i>=10:\n",
    "        g1_classifier.append(4)\n",
    "      elif i<=9 and i>=0:\n",
    "        g1_classifier.append(5)\n",
    "\n",
    "    return g1_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nzgT8YaxNeOP"
   },
   "outputs": [],
   "source": [
    "class feature_selection:\n",
    "    def __init__(self, xdata, ydata, task = 'regression'):\n",
    "      self.xdata = xdata\n",
    "      self.ydata = np.asarray(ydata)\n",
    "      self.task = task\n",
    "\n",
    "\n",
    "    def random_forest(self):\n",
    "      sel = SelectFromModel(RandomForestClassifier(n_estimators = 100))\n",
    "      sel.fit(self.xdata, self.ydata)\n",
    "      sel.get_support()\n",
    "      selected_feat = self.xdata.columns[(sel.get_support())]\n",
    "      selected_data = self.xdata[selected_feat]\n",
    "      return selected_feat \n",
    "\n",
    "    def mutual_info(self):\n",
    "      if (self.task == 'regression'):\n",
    "        selector = SelectKBest(mutual_info_regression, k =10)\n",
    "        selector.fit(self.xdata, self.ydata)\n",
    "        selected_feat = self.xdata.columns[selector.get_support()]\n",
    "        print(\"mutual\",selected_feat)\n",
    "      else:\n",
    "        selector = SelectKBest(mutual_info_classif, k =10)\n",
    "        selector.fit(self.xdata, self.ydata)\n",
    "        selected_feat = self.xdata.columns[selector.get_support()]\n",
    "        print(\"mutual\",selected_feat)\n",
    "      selected_data = self.xdata[selected_feat]\n",
    "      return selected_feat \n",
    "\n",
    "    def chi_squared(self):\n",
    "      selector = SelectKBest(chi2, k = 16)\n",
    "      selector.fit(self.xdata, self.ydata)\n",
    "      selected_feat = self.xdata.columns[selector.get_support()]\n",
    "      print(\"chi_squared\",selected_feat)\n",
    "      selected_data = self.xdata[selected_feat]\n",
    "      return selected_feat \n",
    "\n",
    "    def sequential_feature(self):\n",
    "      sfs_selector = SequentialFeatureSelector(estimator=LinearRegression(), n_features_to_select = 10, cv = 6, direction ='backward')\n",
    "      sfs_selector.fit(self.xdata,self.ydata)\n",
    "      col_req_list = self.xdata.columns[sfs_selector.get_support()]\n",
    "      print(\"sequential_feature\",col_req_list)\n",
    "      selected_data = self.xdata[col_req_list]\n",
    "      return col_req_list\n",
    "\n",
    "    def rfe(self):\n",
    "      rfe_selector = RFE(estimator=LinearRegression(),n_features_to_select = 15, step = 2)\n",
    "      rfe_selector.fit(self.xdata,self.ydata)\n",
    "      col_req_list = self.xdata.columns[rfe_selector.get_support()]\n",
    "      print(\"rfe\",col_req_list)\n",
    "      selected_data = self.xdata[col_req_list]\n",
    "      return col_req_list \n",
    "\n",
    "    def pca_features(self):\n",
    "      pca = PCA()\n",
    "      X_transformed = pca.fit_transform(self.xdata)\n",
    "      return pca,X_transformed\n",
    "\n",
    "\n",
    "    def fisher_features(self):\n",
    "      lda = LinearDiscriminantAnalysis()\n",
    "      X_transformed = lda.fit_transform(self.xdata,self.ydata)\n",
    "      return lda,X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6sZRH9GXUUgx"
   },
   "outputs": [],
   "source": [
    "class baseline_regression:\n",
    "\n",
    "    def __init__(self, xtrain,ytrain,xtest,ytest):\n",
    "      self.xtrain = np.asarray(xtrain)\n",
    "      self.ytrain = np.asarray(ytrain)\n",
    "      self.xtest = np.asarray(xtest)\n",
    "      self.ytest = np.asarray(ytest)\n",
    "\n",
    "\n",
    "    def trivial_system(self):\n",
    "      g1_trivial = np.mean(self.ytrain)\n",
    "      g1_trivial_pred = [g1_trivial]*len(self.ytest)\n",
    "      mse = mean_squared_error(self.ytest, g1_trivial_pred)\n",
    "      rmse =  mean_squared_error(self.ytest, g1_trivial_pred, squared=False)\n",
    "      r2 = r2_score(self.ytest, g1_trivial_pred)    \n",
    "      return rmse,mse,r2 \n",
    "\n",
    "    def linear_reg(self):\n",
    "      reg = LinearRegression().fit(self.xtrain, self.ytrain)\n",
    "      y_pred_linear_baseline = reg.predict(self.xtest)\n",
    "      mse = mean_squared_error(self.ytest, y_pred_linear_baseline)\n",
    "      rmse =  mean_squared_error(self.ytest, y_pred_linear_baseline, squared=False)\n",
    "      r2 = r2_score(self.ytest, y_pred_linear_baseline)    \n",
    "      return rmse,mse,r2 \n",
    "\n",
    "\n",
    "    def one_nn(self):\n",
    "      nbrs = NearestNeighbors(n_neighbors=1).fit(self.xtrain)\n",
    "      distances, indices = nbrs.kneighbors(self.xtest)\n",
    "      y_pred_1nn = self.ytrain[indices]\n",
    "      mse = mean_squared_error(self.ytest, y_pred_1nn)\n",
    "      rmse =  mean_squared_error(self.ytest, y_pred_1nn, squared=False)\n",
    "      r2 = r2_score(self.ytest, y_pred_1nn)    \n",
    "      return rmse,mse,r2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wCdMjQcqzL-d"
   },
   "outputs": [],
   "source": [
    "class regression:\n",
    "\n",
    "  def __init__(self, xtrain,ytrain,xtest,ytest):\n",
    "    self.xtrain = np.asarray(xtrain)\n",
    "    self.ytrain = np.asarray(ytrain)\n",
    "    self.xtest = np.asarray(xtest)\n",
    "    self.ytest = np.asarray(ytest)\n",
    "\n",
    "  def svr(self):\n",
    "    regrassor = SVR(kernel='linear')\n",
    "    target = self.ytrain.reshape(-1,1)\n",
    "    regrassor.fit(self.xtrain, target)\n",
    "    y_pred = regrassor.predict(self.xtest)\n",
    "    y_pred = y_pred.reshape(-1,1)\n",
    "    mse = mean_squared_error(self.ytest, y_pred)\n",
    "    rmse =  mean_squared_error(self.ytest, y_pred, squared=False)\n",
    "    r2 = r2_score(self.ytest, y_pred)    \n",
    "    return rmse,mse,r2\n",
    "  \n",
    "  def ann(self):\n",
    "    regr = MLPRegressor(random_state=1, max_iter=500).fit(self.xtrain, self.ytrain)\n",
    "    y_pred = regr.predict(self.xtest)\n",
    "    mse = mean_squared_error(self.ytest, y_pred)\n",
    "    rmse =  mean_squared_error(self.ytest, y_pred, squared=False)\n",
    "    r2 = r2_score(self.ytest, y_pred)    \n",
    "    return rmse,mse,r2\n",
    "\n",
    "  def gradient_boost(self):\n",
    "    reg = ensemble.GradientBoostingRegressor().fit(self.xtrain, self.ytrain)\n",
    "    y_pred = reg.predict(self.xtest)\n",
    "    mse = mean_squared_error(self.ytest, y_pred)\n",
    "    rmse =  mean_squared_error(self.ytest, y_pred, squared=False)\n",
    "    r2 = r2_score(self.ytest, y_pred)    \n",
    "    return rmse,mse,r2\n",
    "\n",
    "  def knn_regression(self):\n",
    "    k_list=np.arange(1,80,1)\n",
    "    knn_dict={}\n",
    "    knn_list = []\n",
    "    for i in k_list:\n",
    "      knn=KNeighborsRegressor(n_neighbors=int(i))\n",
    "      model_knn=knn.fit(self.xtrain,self.ytrain)\n",
    "      y_knn_pred=model_knn.predict(self.xtest)\n",
    "      mse=mean_squared_error(self.ytest,y_knn_pred)\n",
    "      knn_dict[i]=mse\n",
    "      knn_list.append(mse)\n",
    "\n",
    "    min_val = min(knn_list)\n",
    "    k_size = knn_list.index(min_val)\n",
    "    knn=KNeighborsRegressor(n_neighbors=k_size)\n",
    "    model_knn=knn.fit(self.xtrain,self.ytrain)\n",
    "    y_pred=model_knn.predict(self.xtest)\n",
    "    mse = mean_squared_error(self.ytest, y_pred)\n",
    "    rmse =  mean_squared_error(self.ytest, y_pred, squared=False)\n",
    "    r2 = r2_score(self.ytest, y_pred)    \n",
    "    return rmse,mse,r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u_-ubUegm6wr"
   },
   "outputs": [],
   "source": [
    "class regression_regularization:\n",
    "\n",
    "  def __init__(self, xtrain,ytrain,xtest,ytest):\n",
    "    self.xtrain = np.asarray(xtrain)\n",
    "    self.ytrain = np.asarray(ytrain)\n",
    "    self.xtest = np.asarray(xtest)\n",
    "    self.ytest = np.asarray(ytest)\n",
    "\n",
    "  def svr(self):\n",
    "    regrassor = SVR(kernel='linear',C=3)\n",
    "    target = self.ytrain.reshape(-1,1)\n",
    "    regrassor.fit(self.xtrain, target)\n",
    "    y_pred = regrassor.predict(self.xtest)\n",
    "    y_pred = y_pred.reshape(-1,1)\n",
    "    mse = mean_squared_error(self.ytest, y_pred)\n",
    "    rmse =  mean_squared_error(self.ytest, y_pred, squared=False)\n",
    "    r2 = r2_score(self.ytest, y_pred)    \n",
    "    return rmse,mse,r2\n",
    "  \n",
    "  def ann(self):\n",
    "    regr = MLPRegressor(random_state=1, max_iter=500, alpha = 0.1).fit(self.xtrain, self.ytrain)\n",
    "    y_pred = regr.predict(self.xtest)\n",
    "    mse = mean_squared_error(self.ytest, y_pred)\n",
    "    rmse =  mean_squared_error(self.ytest, y_pred, squared=False)\n",
    "    r2 = r2_score(self.ytest, y_pred)    \n",
    "    return rmse,mse,r2\n",
    "\n",
    "  def ridge_regression(self):\n",
    "    rr = Ridge(alpha=0.1)\n",
    "    rr.fit(self.xtrain, self.ytrain) \n",
    "    y_pred_lr = rr.predict(self.xtest)\n",
    "    mse_rr = mean_squared_error(self.ytest, y_pred_lr)\n",
    "    rmse_rr =  mean_squared_error(self.ytest, y_pred_lr, squared=False)\n",
    "    r2_rr = r2_score(self.ytest, y_pred_lr) \n",
    "\n",
    "    return rmse_rr,mse_rr,r2_rr\n",
    "\n",
    "  def lasso_regression(self):  \n",
    "    lr = Lasso(alpha=0.1)\n",
    "    lr.fit(self.xtrain, self.ytrain) \n",
    "    y_pred = lr.predict(self.xtest)\n",
    "    mse_lr = mean_squared_error(self.ytest, y_pred)\n",
    "    rmse_lr =  mean_squared_error(self.ytest, y_pred, squared=False)\n",
    "    r2_lr = r2_score(self.ytest, y_pred) \n",
    "\n",
    "    return mse_lr,rmse_lr,r2_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V1YtpkKlHIY6"
   },
   "outputs": [],
   "source": [
    "class baseline_classification:\n",
    "\n",
    "    def __init__(self, xtrain,ytrain,xtest,ytest):\n",
    "      self.xtrain = np.asarray(xtrain)\n",
    "      self.ytrain = np.asarray(ytrain)\n",
    "      self.xtest = np.asarray(xtest)\n",
    "      self.ytest = np.asarray(ytest)\n",
    "\n",
    "\n",
    "    def trivial_system(self):\n",
    "      priors = {}\n",
    "      priors_1 = 0\n",
    "      priors_2 = 0\n",
    "      priors_3 = 0\n",
    "      priors_4 = 0\n",
    "      priors_5 = 0\n",
    "\n",
    "      for i in self.ytrain:\n",
    "        if i==1:\n",
    "          priors_1+=1\n",
    "        elif i==2:\n",
    "          priors_2+=1\n",
    "        elif i==3:\n",
    "          priors_3+=1\n",
    "        elif i==4:\n",
    "          priors_4+=1\n",
    "        else:\n",
    "          priors_5+=1\n",
    "  \n",
    "      priors[1] = priors_1/len(self.ytrain)\n",
    "      priors[2] = priors_2/len(self.ytrain)\n",
    "      priors[3] = priors_3/len(self.ytrain)\n",
    "      priors[4] = priors_4/len(self.ytrain)\n",
    "      priors[5] = priors_5/len(self.ytrain)\n",
    "\n",
    "      prior_list = list(priors.values())  \n",
    "      accuracy = 0\n",
    "      macro_f1_score = 0\n",
    "\n",
    "      for k in range(10):\n",
    "        g1_classifier_trivial = []\n",
    "        for i in range(len(self.ytest)):\n",
    "          g1_classifier_trivial.append(np.random.choice(np.arange(1, 6), p=prior_list))\n",
    "\n",
    "        accuracy += accuracy_score(g1_classifier_trivial, self.ytest)\n",
    "        macro_f1_score += f1_score(g1_classifier_trivial, self.ytest, average='macro')\n",
    "\n",
    "      accuracy_avg = accuracy/10\n",
    "      macro_f1_score_avg = macro_f1_score/10\n",
    "      cf = confusion_matrix(g1_classifier_trivial, self.ytest)\n",
    "      return accuracy_avg,macro_f1_score_avg,cf\n",
    "\n",
    "    \n",
    "\n",
    "    def nearest_means(self):\n",
    "      clf = NearestCentroid()\n",
    "      clf.fit(self.xtrain, self.ytrain)\n",
    "      baseline_classifier = clf.predict(np.asarray(self.xtest))\n",
    "      acc = accuracy_score(self.ytest, baseline_classifier)\n",
    "      macro_f1_score = f1_score(baseline_classifier, self.ytest, average='macro')\n",
    "      cf = confusion_matrix(baseline_classifier, self.ytest)\n",
    "      return acc,macro_f1_score,cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bfBBGNrTZDjk"
   },
   "outputs": [],
   "source": [
    "class classification:\n",
    "\n",
    "  def __init__(self, xtrain,ytrain,xtest,ytest):\n",
    "    self.xtrain = np.asarray(xtrain)\n",
    "    self.ytrain = np.asarray(ytrain)\n",
    "    self.xtest = np.asarray(xtest)\n",
    "    self.ytest = np.asarray(ytest)\n",
    "\n",
    "  def logistic_reg(self):\n",
    "    clf = LogisticRegression(random_state=0).fit(self.xtrain, self.ytrain)\n",
    "    pred_class = clf.predict(self.xtest)\n",
    "    acc = accuracy_score(self.ytest, pred_class)\n",
    "    macro_f1_score = f1_score(pred_class, self.ytest, average='macro')\n",
    "    cf = confusion_matrix(pred_class, self.ytest)\n",
    "    return acc,macro_f1_score,cf\n",
    "\n",
    "  def svm_classification(self):\n",
    "    linear = svm.SVC(kernel='linear', C=1, decision_function_shape='ovo').fit(self.xtrain, self.ytrain)\n",
    "    rbf = svm.SVC(kernel='rbf', gamma=1, C=1, decision_function_shape='ovo').fit(self.xtrain, self.ytrain)\n",
    "    poly = svm.SVC(kernel='poly', degree=3, C=1, decision_function_shape='ovo').fit(self.xtrain, self.ytrain)\n",
    "    sig = svm.SVC(kernel='sigmoid', C=1, decision_function_shape='ovo').fit(self.xtrain, self.ytrain)\n",
    "\n",
    "    linear_pred = linear.predict(self.xtest)\n",
    "    poly_pred = poly.predict(self.xtest)\n",
    "    rbf_pred = rbf.predict(self.xtest)\n",
    "    sig_pred = sig.predict(self.xtest)\n",
    "\n",
    "    accuracy_list = []\n",
    "    accuracy_list.append(linear.score(self.xtest, self.ytest))\n",
    "    accuracy_list.append(poly.score(self.xtest, self.ytest))\n",
    "    accuracy_list.append(rbf.score(self.xtest, self.ytest))\n",
    "    accuracy_list.append(sig.score(self.xtest, self.ytest))\n",
    "\n",
    "    highest_score = max(accuracy_list)\n",
    "    ind = accuracy_list.index(highest_score)\n",
    "\n",
    "    if(ind == 0):\n",
    "      acc = accuracy_score(self.ytest, linear_pred)\n",
    "      macro_f1_score = f1_score(linear_pred, self.ytest, average='macro')\n",
    "      cf = confusion_matrix(linear_pred, self.ytest)\n",
    "      print(\"Linear Kernel\")\n",
    "      plt.figure(figsize = (7,7))\n",
    "      sns.heatmap(cf, annot=True, cmap='Blues')\n",
    "      plt.show()\n",
    "      return acc,macro_f1_score,cf\n",
    "    elif(ind==1):\n",
    "      acc = accuracy_score(self.ytest, poly_pred)\n",
    "      macro_f1_score = f1_score(poly_pred, self.ytest, average='macro')\n",
    "      cf = confusion_matrix(poly_pred, self.ytest)\n",
    "      print(\"Poly Kernel\")\n",
    "      plt.figure(figsize = (7,7))\n",
    "      sns.heatmap(cf, annot=True, cmap='Blues')\n",
    "      plt.show()\n",
    "      return acc,macro_f1_score,cf\n",
    "    elif(ind==2):\n",
    "      acc = accuracy_score(self.ytest, rbf_pred)\n",
    "      macro_f1_score = f1_score(rbf_pred, self.ytest, average='macro')\n",
    "      cf = confusion_matrix(rbf_pred, self.ytest)\n",
    "      print(\"RBF Kernel\")\n",
    "      plt.figure(figsize = (7,7))\n",
    "      sns.heatmap(cf, annot=True, cmap='Blues')\n",
    "      plt.show()\n",
    "      return acc,macro_f1_score,cf\n",
    "    else:\n",
    "      acc = accuracy_score(self.ytest, sig_pred)\n",
    "      macro_f1_score = f1_score(sig_pred, self.ytest, average='macro')\n",
    "      cf = confusion_matrix(sig_pred, self.ytest)\n",
    "      print(\"Sigmoid Kernel\")\n",
    "      plt.figure(figsize = (7,7))\n",
    "      sns.heatmap(cf, annot=True, cmap='Blues')\n",
    "      plt.show()\n",
    "      return acc,macro_f1_score,cf\n",
    "\n",
    "  def knn_classification(self):\n",
    "    # k_list=np.arange(1,80,1)\n",
    "    k_list =  list(range(1,80))\n",
    "    knn_dict={} # To store k and mse pairs\n",
    "    knn_list = []\n",
    "    for i in k_list:\n",
    "      knn=KNeighborsClassifier(n_neighbors = int(i))\n",
    "      model_knn=knn.fit(self.xtrain,self.ytrain)\n",
    "      y_knn_pred=model_knn.predict(self.xtest)\n",
    "\n",
    "      mse=mean_squared_error(self.ytest,y_knn_pred)\n",
    "      knn_dict[i]=mse\n",
    "      knn_list.append(mse)\n",
    "\n",
    "    plt.plot(knn_list)\n",
    "    min_val = min(knn_list)\n",
    "    neigbor_val = knn_list.index(min_val)\n",
    "    knn=KNeighborsClassifier(n_neighbors = neigbor_val)\n",
    "    model_knn=knn.fit(self.xtrain,self.ytrain)\n",
    "    y_knn_pred=model_knn.predict(self.xtest)\n",
    "    acc = accuracy_score(self.ytest, y_knn_pred)\n",
    "    macro_f1_score = f1_score(y_knn_pred, self.ytest, average='macro')\n",
    "    cf = confusion_matrix(y_knn_pred, self.ytest)\n",
    "    return acc,macro_f1_score,cf\n",
    "    \n",
    "\n",
    "  def gaussian_process_classifier(self):\n",
    "    kernel = 1.0 * RBF(1.0)\n",
    "    gpc = GaussianProcessClassifier(kernel=kernel,random_state=0).fit(self.xtrain, self.ytrain)\n",
    "    x_test_temp = self.xtest.reshape(1,-1)\n",
    "    y_pred = gpc.predict(self.xtest)\n",
    "    acc = accuracy_score(self.ytest, y_pred)\n",
    "    macro_f1_score = f1_score(y_pred, self.ytest, average='macro')\n",
    "    cf = confusion_matrix(y_pred, self.ytest)\n",
    "    return acc,macro_f1_score,cf\n",
    "\n",
    "  def random_forest_classifier(self):\n",
    "    clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "    clf.fit(self.xtrain, self.ytrain)\n",
    "    y_pred = clf.predict(self.xtest)\n",
    "    acc = accuracy_score(self.ytest, y_pred)\n",
    "    macro_f1_score = f1_score(y_pred, self.ytest, average='macro')\n",
    "    cf = confusion_matrix(y_pred, self.ytest)\n",
    "    return acc,macro_f1_score,cf\n",
    "  \n",
    "  def mlp_classifier(self):\n",
    "    clf = MLPClassifier(random_state=1, max_iter=300).fit(self.xtrain, self.ytrain)\n",
    "    y_pred = clf.predict(self.xtest)\n",
    "    acc = accuracy_score(self.ytest, y_pred)\n",
    "    macro_f1_score = f1_score(y_pred, self.ytest, average='macro')\n",
    "    cf = confusion_matrix(y_pred, self.ytest)\n",
    "    return acc,macro_f1_score,cf\n",
    "\n",
    "  def naives_bayesian(self):\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(self.xtrain, self.ytrain)\n",
    "    y_pred = clf.predict(self.xtest)\n",
    "    acc = accuracy_score(self.ytest, y_pred)\n",
    "    macro_f1_score = f1_score(y_pred, self.ytest, average='macro')\n",
    "    cf = confusion_matrix(y_pred, self.ytest)\n",
    "    return acc,macro_f1_score,cf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "46n59T9agQea"
   },
   "outputs": [],
   "source": [
    "class classification_regularization:\n",
    "\n",
    "  def __init__(self, xtrain,ytrain,xtest,ytest):\n",
    "    self.xtrain = np.asarray(xtrain)\n",
    "    self.ytrain = np.asarray(ytrain)\n",
    "    self.xtest = np.asarray(xtest)\n",
    "    self.ytest = np.asarray(ytest)\n",
    "\n",
    "  def logistic_reg_l1(self):\n",
    "    clf = LogisticRegression(C=1, penalty='l1', solver='liblinear').fit(self.xtrain, self.ytrain)\n",
    "    pred_class = clf.predict(self.xtest)\n",
    "    acc = accuracy_score(self.ytest, pred_class)\n",
    "    macro_f1_score = f1_score(pred_class, self.ytest, average='macro')\n",
    "    cf = confusion_matrix(pred_class, self.ytest)\n",
    "    return acc,macro_f1_score,cf\n",
    "\n",
    "  \n",
    "  def logistic_reg_l2(self):\n",
    "    clf = LogisticRegression(random_state=0,penalty = 'l2').fit(self.xtrain, self.ytrain)\n",
    "    pred_class = clf.predict(self.xtest)\n",
    "    acc = accuracy_score(self.ytest, pred_class)\n",
    "    macro_f1_score = f1_score(pred_class, self.ytest, average='macro')\n",
    "    cf = confusion_matrix(pred_class, self.ytest)\n",
    "    return acc,macro_f1_score,cf\n",
    "\n",
    "\n",
    "  def svm_classification(self):\n",
    "    linear = svm.SVC(kernel='linear', C=3, decision_function_shape='ovo').fit(self.xtrain, self.ytrain)\n",
    "    rbf = svm.SVC(kernel='rbf', gamma=1, C=3, decision_function_shape='ovo').fit(self.xtrain, self.ytrain)\n",
    "    poly = svm.SVC(kernel='poly', degree=3, C=3, decision_function_shape='ovo').fit(self.xtrain, self.ytrain)\n",
    "    sig = svm.SVC(kernel='sigmoid', C=3, decision_function_shape='ovo').fit(self.xtrain, self.ytrain)\n",
    "\n",
    "    linear_pred = linear.predict(self.xtest)\n",
    "    poly_pred = poly.predict(self.xtest)\n",
    "    rbf_pred = rbf.predict(self.xtest)\n",
    "    sig_pred = sig.predict(self.xtest)\n",
    "\n",
    "    accuracy_list = []\n",
    "    accuracy_list.append(linear.score(self.xtest, self.ytest))\n",
    "    accuracy_list.append(poly.score(self.xtest, self.ytest))\n",
    "    accuracy_list.append(rbf.score(self.xtest, self.ytest))\n",
    "    accuracy_list.append(sig.score(self.xtest, self.ytest))\n",
    "\n",
    "    highest_score = max(accuracy_list)\n",
    "    ind = accuracy_list.index(highest_score)\n",
    "\n",
    "    if(ind == 0):\n",
    "      acc = accuracy_score(self.ytest, linear_pred)\n",
    "      macro_f1_score = f1_score(linear_pred, self.ytest, average='macro')\n",
    "      cf = confusion_matrix(linear_pred, self.ytest)\n",
    "      print(\"Linear Kernel\")\n",
    "      plt.figure(figsize = (7,7))\n",
    "      sns.heatmap(cf, annot=True, cmap='Blues')\n",
    "      plt.show()\n",
    "      return acc,macro_f1_score,cf\n",
    "    elif(ind==1):\n",
    "      acc = accuracy_score(self.ytest, poly_pred)\n",
    "      macro_f1_score = f1_score(poly_pred, self.ytest, average='macro')\n",
    "      cf = confusion_matrix(poly_pred, self.ytest)\n",
    "      print(\"Poly Kernel\")\n",
    "      plt.figure(figsize = (7,7))\n",
    "      sns.heatmap(cf, annot=True, cmap='Blues')\n",
    "      plt.show()\n",
    "      return acc,macro_f1_score,cf\n",
    "    elif(ind==2):\n",
    "      acc = accuracy_score(self.ytest, rbf_pred)\n",
    "      macro_f1_score = f1_score(rbf_pred, self.ytest, average='macro')\n",
    "      cf = confusion_matrix(rbf_pred, self.ytest)\n",
    "      print(\"RBF Kernel\")\n",
    "      plt.figure(figsize = (7,7))\n",
    "      sns.heatmap(cf, annot=True, cmap='Blues')\n",
    "      plt.show()\n",
    "      return acc,macro_f1_score,cf\n",
    "    else:\n",
    "      acc = accuracy_score(self.ytest, sig_pred)\n",
    "      macro_f1_score = f1_score(sig_pred, self.ytest, average='macro')\n",
    "      print(\"Sigmoid Kernel\")\n",
    "      cf = confusion_matrix(sig_pred, self.ytest)\n",
    "      plt.figure(figsize = (7,7))\n",
    "      sns.heatmap(cf, annot=True, cmap='Blues')\n",
    "      plt.show()\n",
    "      return acc,macro_f1_score,cf\n",
    "  \n",
    "  def mlp_classifier(self):\n",
    "    clf = MLPClassifier(random_state=1, max_iter=300,alpha = 0.1).fit(self.xtrain, self.ytrain)\n",
    "    y_pred = clf.predict(self.xtest)\n",
    "    acc = accuracy_score(self.ytest, y_pred)\n",
    "    macro_f1_score = f1_score(y_pred, self.ytest, average='macro')\n",
    "    cf = confusion_matrix(y_pred, self.ytest)\n",
    "    plt.figure(figsize = (7,7))\n",
    "    sns.heatmap(cf, annot=True, cmap='Blues')\n",
    "    plt.show()\n",
    "    return acc,macro_f1_score,cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6L6o-2yZtbLv"
   },
   "outputs": [],
   "source": [
    "class regression_analysis:\n",
    "\n",
    "  def __init__(self,x_train,y_train,x_test,y_test):\n",
    "    self.x_train = x_train\n",
    "    self.y_train = y_train\n",
    "    self.x_test = x_test\n",
    "    self.y_test = y_test\n",
    "\n",
    "  \n",
    "  def perform_regression(self):\n",
    "\n",
    "    reg_model = regression(self.x_train,self.y_train,self.x_test,self.y_test)\n",
    "\n",
    "    rmse_svr,mse_svr,r2_svr = reg_model.svr()\n",
    "    print(\"Performance of SVR\")\n",
    "    print(\"rmse of SVR \", rmse_svr)\n",
    "    print(\"mse of SVR \", mse_svr)\n",
    "    print(\"r2 score of SVR \", r2_svr)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    #ANN\n",
    "\n",
    "    rmse_ann,mse_ann,r2_ann = reg_model.ann()\n",
    "    print(\"Performance of ANN\")\n",
    "    print(\"rmse of ANN \", rmse_ann,)\n",
    "    print(\"mse of ANN \", mse_ann)\n",
    "    print(\"r2 score of ANN \", r2_ann)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    #Gradient boost regressor\n",
    "\n",
    "    rmse_lr,mse_lr,r2_lr = reg_model.gradient_boost()\n",
    "    print(\"Performance of Gradient boost regressor\")\n",
    "    print(\"rmse of Gradient boost regressorn \", rmse_lr,)\n",
    "    print(\"mse of Gradient boost regressor \", mse_lr)\n",
    "    print(\"r2 score of Gradient boost regressor \", r2_lr)\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "    #knn regression\n",
    "    rmse_knn,mse_knn,r2_knn = reg_model.knn_regression()\n",
    "    print(\"Performance of KNN\")\n",
    "    print(\"rmse of KNN \", rmse_knn,)\n",
    "    print(\"mse of KNN \", mse_knn)\n",
    "    print(\"r2 score of KNN\", r2_knn)\n",
    "    print(\"\\n\")\n",
    "\n",
    "  def perform_regression_regularization(self):\n",
    "\n",
    "    reg_model = regression_regularization(self.x_train,self.y_train,self.x_test,self.y_test)\n",
    "\n",
    "    rmse_svr,mse_svr,r2_svr = reg_model.svr()\n",
    "    print(\"Performance of SVR\")\n",
    "    print(\"rmse of SVR \", rmse_svr)\n",
    "    print(\"mse of SVR \", mse_svr)\n",
    "    print(\"r2 score of SVR \", r2_svr)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    #ANN\n",
    "    print(\"Performance of ANN\")\n",
    "    rmse_ann,mse_ann,r2_ann = reg_model.ann()\n",
    "    print(\"rmse of ANN \", rmse_ann,)\n",
    "    print(\"mse of ANN \", mse_ann)\n",
    "    print(\"r2 score of ANN \", r2_ann)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    #Ridge regression\n",
    "    print(\"Performance of Ridge Regression\")\n",
    "    rmse_lr,mse_lr,r2_lr = reg_model.ridge_regression()\n",
    "    print(\"rmse of ridge regression \", rmse_lr,)\n",
    "    print(\"mse of ridge regression \", mse_lr)\n",
    "    print(\"r2 score of ridge regression \", r2_lr)\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "    #Lasso regression\n",
    "    print(\"Performance of Lasso Regression\")\n",
    "    rmse_knn,mse_knn,r2_knn = reg_model.lasso_regression()\n",
    "    print(\"rmse of lasso regression\", rmse_knn,)\n",
    "    print(\"mse of lasso regression \", mse_knn)\n",
    "    print(\"r2 score of lasso regression\", r2_knn)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xKyESjJp1BDX"
   },
   "outputs": [],
   "source": [
    "def regresion_feature_selection(x_train,y_train,x_test,y_test,flag = 'regularize',mission = None):\n",
    "\n",
    "    feature_selected = feature_selection(x_train,y_train)\n",
    "\n",
    "    fs_random = feature_selected.random_forest()\n",
    "    fs_mututal = feature_selected.mutual_info()\n",
    "    fs_chi = feature_selected.chi_squared()\n",
    "    fs_seq = feature_selected.sequential_feature()\n",
    "    fs_rfe = feature_selected.rfe()\n",
    "    pca_g1, x_train_tranformed_pca = feature_selected.pca_features()\n",
    "    x_test_tranformed = pca_g1.transform(x_test)\n",
    "\n",
    "\n",
    "    print(\"The performance of different regression models on features selected using random forest\")\n",
    "    reg_analysis = regression_analysis(x_train[fs_random],y_train,x_test[fs_random],y_test)\n",
    "    if (flag=='regularize'):\n",
    "      reg_analysis.perform_regression_regularization()\n",
    "    else:\n",
    "      reg_analysis.perform_regression()\n",
    "    print(\"\\n\")\n",
    "\n",
    "  \n",
    "    print(\"The performance of different regression models on features selected using Mutual Information\")\n",
    "    reg_analysis = regression_analysis(x_train[fs_mututal],y_train,x_test[fs_mututal],y_test)\n",
    "    if (flag=='regularize'):\n",
    "      reg_analysis.perform_regression_regularization()\n",
    "    else:\n",
    "      reg_analysis.perform_regression()\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"The performance of different regression models on features selected using Chi squared\")\n",
    "    reg_analysis = regression_analysis(x_train[fs_chi],y_train,x_test[fs_chi],y_test)\n",
    "    if (flag=='regularize'):\n",
    "      reg_analysis.perform_regression_regularization()\n",
    "      \n",
    "    else:\n",
    "      reg_analysis.perform_regression()\n",
    "    print(\"\\n\")\n",
    "  \n",
    "\n",
    "    print(\"The performance of different regression models on features selected using Sequential Features\")\n",
    "    reg_analysis = regression_analysis(x_train[fs_seq],y_train,x_test[fs_seq],y_test)\n",
    "    if (flag=='regularize'):\n",
    "       reg_analysis.perform_regression_regularization()\n",
    "    else:\n",
    "      reg_analysis.perform_regression()\n",
    "    print(\"\\n\")\n",
    "  \n",
    "    print(\"The performance of different regression models on features selected using RFE\")\n",
    "    reg_analysis = regression_analysis(x_train[fs_rfe],y_train,x_test[fs_rfe],y_test)\n",
    "    if (flag=='regularize'):\n",
    "      reg_analysis.perform_regression_regularization()\n",
    "    else:\n",
    "      reg_analysis.perform_regression()\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "    if(mission !=3):\n",
    "      print(\"The performance of different regression models on features selected using PCA\")\n",
    "      reg_analysis = regression_analysis(x_train_tranformed_pca,y_train,x_test_tranformed,y_test)\n",
    "      if (flag=='regularize'):\n",
    "        reg_analysis.perform_regression_regularization()\n",
    "      else:\n",
    "        reg_analysis.perform_regression()\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_qzEG3VQxaYW"
   },
   "outputs": [],
   "source": [
    "class classification_analysis:\n",
    "\n",
    "  def __init__(self,x_train,y_train,x_test,y_test):\n",
    "    self.x_train = x_train\n",
    "    self.y_train = y_train\n",
    "    self.x_test = x_test\n",
    "    self.y_test = y_test\n",
    "\n",
    "  def perform_classification(self):\n",
    "    class_model = classification(self.x_train,self.y_train,self.x_test,self.y_test)\n",
    "\n",
    "    acc,macro_f1_score,cf = class_model.logistic_reg()\n",
    "    print(\"Accuracy of logistic regression system \", acc)\n",
    "    print(\"Macro f1 score of logistic regression system \", macro_f1_score)\n",
    "    print(\"Confusion matrix of logistic regression system : \\n\", cf)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    acc,macro_f1_score,cf = class_model.svm_classification()\n",
    "    print(\"Accuracy of SVM system \", acc)\n",
    "    print(\"Macro f1 score of SVM system \", macro_f1_score)\n",
    "    print(\"Confusion matrix of SVM system : \\n\", cf)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    acc,macro_f1_score,cf = class_model.gaussian_process_classifier()\n",
    "    print(\"Accuracy of Gaussian process classifier \", acc)\n",
    "    print(\"Macro f1 score of Gaussian process classifier\", macro_f1_score)\n",
    "    print(\"Confusion matrix of Gaussian process classifier : \\n\", cf)  \n",
    "    print(\"\\n\")\n",
    "\n",
    "    acc,macro_f1_score,cf =  class_model.random_forest_classifier()\n",
    "    print(\"Accuracy of random forest classifier \", acc)\n",
    "    print(\"Macro f1 score of random forest classifier\", macro_f1_score)\n",
    "    print(\"Confusion matrix of random forest classifier : \\n\", cf)  \n",
    "    print(\"\\n\")\n",
    "\n",
    "    acc,macro_f1_score,cf =  class_model.mlp_classifier()\n",
    "    print(\"Accuracy of MLP classifier \", acc)\n",
    "    print(\"Macro f1 score of MLP classifier\", macro_f1_score)\n",
    "    print(\"Confusion matrix of MLP classifier : \\n\", cf)  \n",
    "    print(\"\\n\")\n",
    "\n",
    "    acc,macro_f1_score,cf =  class_model.naives_bayesian()\n",
    "    print(\"Accuracy of naives bayesian classifier \", acc)\n",
    "    print(\"Macro f1 score of naives bayesian classifier\", macro_f1_score)\n",
    "    print(\"Confusion matrix of naives bayesian classifier : \\n\", cf) \n",
    "    print(\"\\n\")\n",
    "\n",
    "    acc,macro_f1_score,cf = class_model.knn_classification()\n",
    "    print(\"Accuracy of KNN system \", acc)\n",
    "    print(\"Macro f1 score of KNN system \", macro_f1_score)\n",
    "    print(\"Confusion matrix of KNN system : \\n\", cf)   \n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "  def perform_classification_regularisation(self):\n",
    "    class_model = classification_regularization(self.x_train,self.y_train,self.x_test,self.y_test)\n",
    "\n",
    "    acc,macro_f1_score,cf = class_model.logistic_reg_l1()\n",
    "    print(\"Accuracy of logistic regression system \", acc)\n",
    "    print(\"Macro f1 score of logistic regression system \", macro_f1_score)\n",
    "    print(\"Confusion matrix of logistic regression system : \\n\", cf)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    acc,macro_f1_score,cf = class_model.logistic_reg_l2()\n",
    "    print(\"Accuracy of logistic regression system \", acc)\n",
    "    print(\"Macro f1 score of logistic regression system \", macro_f1_score)\n",
    "    print(\"Confusion matrix of logistic regression system : \\n\", cf)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    acc,macro_f1_score,cf = class_model.svm_classification()\n",
    "    print(\"Accuracy of SVM system \", acc)\n",
    "    print(\"Macro f1 score of SVM system \", macro_f1_score)\n",
    "    print(\"Confusion matrix of SVM system : \\n\", cf)  \n",
    "    print(\"\\n\")\n",
    "\n",
    "    acc,macro_f1_score,cf =  class_model.mlp_classifier()\n",
    "    print(\"Accuracy of MLP classifier \", acc)\n",
    "    print(\"Macro f1 score of MLP classifier\", macro_f1_score)\n",
    "    print(\"Confusion matrix of MLP classifier : \\n\", cf)   \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b94PEiwI1uxo"
   },
   "outputs": [],
   "source": [
    "def classification_feature_selection(x_train,y_train,x_test,y_test,flag = 'regularize',mission = None):\n",
    "\n",
    "    feature_selected = feature_selection(x_train,y_train,task='classification')\n",
    "\n",
    "    fs_random = feature_selected.random_forest()\n",
    "    fs_mututal = feature_selected.mutual_info()\n",
    "    fs_chi = feature_selected.chi_squared()\n",
    "    fs_seq = feature_selected.sequential_feature()\n",
    "    fs_rfe = feature_selected.rfe()\n",
    "    pca_g1, x_train_tranformed_pca = feature_selected.pca_features()\n",
    "    x_test_tranformed = pca_g1.transform(x_test)\n",
    "    lda_g1, x_train_tranformed_lda = feature_selected.fisher_features()\n",
    "    x_test_tranformed_lda = lda_g1.transform(x_test)\n",
    "\n",
    "\n",
    "    print(\"The performance of different classification models on features selected using random forest\")\n",
    "    class_analysis = classification_analysis(x_train[fs_random],y_train,x_test[fs_random],y_test)\n",
    "    if (flag=='regularize'):\n",
    "      class_analysis.perform_classification_regularisation()\n",
    "    else:\n",
    "      class_analysis.perform_classification()\n",
    "    print(\"\\n\")\n",
    "  \n",
    "    print(\"The performance of different classification models on features selected using Mutual Information\")\n",
    "    class_analysis = classification_analysis(x_train[fs_mututal],y_train,x_test[fs_mututal],y_test)\n",
    "    if (flag=='regularize'):\n",
    "      class_analysis.perform_classification_regularisation()\n",
    "    else:\n",
    "      class_analysis.perform_classification()\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"The performance of different classification models on features selected using Chi squared\")\n",
    "    class_analysis = classification_analysis(x_train[fs_chi],y_train,x_test[fs_chi],y_test)\n",
    "    if (flag=='regularize'):\n",
    "      class_analysis.perform_classification_regularisation()\n",
    "    else:\n",
    "      class_analysis.perform_classification()\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"The performance of different classification models on features selected using Sequential Features\")\n",
    "    class_analysis = classification_analysis(x_train[fs_seq],y_train,x_test[fs_seq],y_test)\n",
    "    if (flag=='regularize'):\n",
    "       class_analysis.perform_classification_regularisation()\n",
    "    else:\n",
    "      class_analysis.perform_classification()\n",
    "    print(\"\\n\")\n",
    "  \n",
    "    print(\"The performance of different classification models on features selected using RFE\")\n",
    "    class_analysis = classification_analysis(x_train[fs_rfe],y_train,x_test[fs_rfe],y_test)\n",
    "    if (flag=='regularize'):\n",
    "      class_analysis.perform_classification_regularisation()\n",
    "    else:\n",
    "      class_analysis.perform_classification()\n",
    "    print(\"\\n\")\n",
    "\n",
    "    if(mission!=3):\n",
    "      print(\"The performance of different classification models on features selected using PCA\")\n",
    "      class_analysis = classification_analysis(x_train_tranformed_pca,y_train,x_test_tranformed,y_test)\n",
    "      if (flag=='regularize'):\n",
    "        class_analysis.perform_classification_regularisation()\n",
    "      else:\n",
    "        class_analysis.perform_classification()\n",
    "    print(\"\\n\")\n",
    "\n",
    "    if(mission!=3):\n",
    "      print(\"The performance of different classification models on features selected using FLD\")\n",
    "      class_analysis = classification_analysis(x_train_tranformed_lda,y_train,x_test_tranformed_lda,y_test)\n",
    "      if (flag=='regularize'):\n",
    "        class_analysis.perform_classification_regularisation()\n",
    "      else:\n",
    "        class_analysis.perform_classification()\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0qSV31qcVVjX"
   },
   "outputs": [],
   "source": [
    "#k fold\n",
    "\n",
    "class k_fold_validation:\n",
    "    def __init__(self, xtrain,ytrain,xtest,ytest):\n",
    "      self.xtrain = xtrain\n",
    "      self.ytrain = ytrain\n",
    "      self.xtest = xtest\n",
    "      self.ytest = ytest\n",
    "\n",
    "    def k_fold_regression(self):\n",
    "\n",
    "      k = 5\n",
    "      kf = KFold(n_splits=k, random_state=None)\n",
    "\n",
    "      rmse_svr_list = []\n",
    "      rmse_ann_list = []\n",
    "      rmse_lr_list = []\n",
    "      rmse_knn_list = []\n",
    "\n",
    "      mse_svr_list = []\n",
    "      mse_ann_list = []\n",
    "      mse_lr_list = []\n",
    "      mse_knn_list = []\n",
    "\n",
    "      r2_svr_list = []\n",
    "      r2_ann_list = []\n",
    "      r2_lr_list = []\n",
    "      r2_knn_list = []\n",
    "\n",
    "      \n",
    "      for train_index , test_index in kf.split(self.xtrain):\n",
    "          X_train , X_test = self.xtrain.iloc[train_index,:],self.xtrain.iloc[test_index,:]\n",
    "          y_train , y_test = self.ytrain[train_index],self.ytrain[test_index]\n",
    "          regression_model = regression(X_train,y_train,X_test,y_test)\n",
    "          rmse_svr,mse_svr,r2_svr = regression_model.svr()\n",
    "          rmse_ann,mse_ann,r2_ann = regression_model.ann()\n",
    "          rmse_lr,mse_lr,r2_lr = regression_model.gradient_boost()\n",
    "          rmse_knn,mse_knn,r2_knn = regression_model.knn_regression()\n",
    "\n",
    "          mse_svr_list.append(mse_svr)\n",
    "          mse_ann_list.append(mse_ann)\n",
    "          mse_lr_list.append(mse_lr)\n",
    "          mse_knn_list.append(mse_knn)\n",
    "\n",
    "          rmse_svr_list.append(rmse_svr)\n",
    "          rmse_ann_list.append(rmse_ann)\n",
    "          rmse_lr_list.append(rmse_lr)\n",
    "          rmse_knn_list.append(rmse_knn)\n",
    "\n",
    "          r2_svr_list.append(r2_svr)\n",
    "          r2_ann_list.append(r2_ann)\n",
    "          r2_lr_list.append(r2_lr)\n",
    "          r2_knn_list.append(r2_knn)\n",
    "\n",
    "      \n",
    "      avg_mse_svr = sum(mse_svr_list)/k\n",
    "      avg_mse_ann = sum(mse_ann_list)/k\n",
    "      avg_mse_lr = sum(mse_lr_list)/k\n",
    "      avg_mse_knn = sum(mse_knn_list)/k\n",
    "\n",
    "      avg_rmse_svr = sum(rmse_svr_list)/k\n",
    "      avg_rmse_ann = sum(rmse_ann_list)/k\n",
    "      avg_rmse_lr = sum(rmse_lr_list)/k\n",
    "      avg_rmse_knn = sum(rmse_knn_list)/k\n",
    "\n",
    "      avg_r2_svr = sum(r2_svr_list)/k\n",
    "      avg_r2_ann = sum(r2_ann_list)/k\n",
    "      avg_r2_lr = sum(r2_lr_list)/k\n",
    "      avg_r2_knn = sum(r2_knn_list)/k\n",
    "\n",
    "\n",
    "      print('Avg MSE, RMSE and r2 score of SVR',avg_mse_svr,avg_rmse_svr,avg_r2_svr)\n",
    "      print('Avg MSE, RMSE and r2 score of ANN',avg_mse_ann,avg_rmse_ann,avg_r2_ann)\n",
    "      print('Avg MSE, RMSE and r2 score of Gradient boost regressor',avg_mse_lr,avg_rmse_lr,avg_r2_lr)\n",
    "      print('Avg MSE, RMSE and r2 score of KNN',avg_mse_knn,avg_rmse_knn,avg_r2_knn)\n",
    "      print(\"\\n\")\n",
    "\n",
    "\n",
    "    def k_fold_regression_regularize(self):\n",
    "\n",
    "      k = 5\n",
    "      kf = KFold(n_splits=k, random_state=None)\n",
    "\n",
    "      rmse_svr_list = []\n",
    "      rmse_ann_list = []\n",
    "      rmse_ridge_list = []\n",
    "      rmse_lasso_list = []\n",
    "\n",
    "      mse_svr_list = []\n",
    "      mse_ann_list = []\n",
    "      mse_ridge_list = []\n",
    "      mse_lasso_list = []\n",
    "\n",
    "      r2_svr_list = []\n",
    "      r2_ann_list = []\n",
    "      r2_ridge_list = []\n",
    "      r2_lasso_list = []\n",
    "\n",
    "      \n",
    "      for train_index , test_index in kf.split(self.xtrain):\n",
    "          X_train , X_test = self.xtrain.iloc[train_index,:],self.xtrain.iloc[test_index,:]\n",
    "          y_train , y_test = self.ytrain[train_index],self.ytrain[test_index]\n",
    "      \n",
    "          regression_model = regression_regularization(X_train,y_train,X_test,y_test)\n",
    "          rmse_svr,mse_svr,r2_svr = regression_model.svr()\n",
    "          rmse_ann,mse_ann,r2_ann = regression_model.ann()\n",
    "          rmse_ridge,mse_ridge,r2_ridge = regression_model.ridge_regression()\n",
    "          rmse_lasso,mse_lasso,r2_lasso = regression_model.lasso_regression()\n",
    "\n",
    "          mse_svr_list.append(mse_svr)\n",
    "          mse_ann_list.append(mse_ann)\n",
    "          mse_ridge_list.append(mse_ridge)\n",
    "          mse_lasso_list.append(mse_lasso)\n",
    "\n",
    "          rmse_svr_list.append(rmse_svr)\n",
    "          rmse_ann_list.append(rmse_ann)\n",
    "          rmse_ridge_list.append(rmse_ridge)\n",
    "          rmse_lasso_list.append(rmse_lasso)\n",
    "\n",
    "\n",
    "          r2_svr_list.append(r2_svr)\n",
    "          r2_ann_list.append(r2_ann)\n",
    "          r2_ridge_list.append(r2_ridge)\n",
    "          r2_lasso_list.append(r2_lasso)\n",
    "\n",
    "      \n",
    "      avg_mse_svr = sum(mse_svr_list)/k\n",
    "      avg_mse_ann = sum(mse_ann_list)/k\n",
    "      avg_mse_ridge = sum(mse_ridge_list)/k\n",
    "      avg_mse_lasso = sum(mse_lasso_list)/k\n",
    "\n",
    "      avg_rmse_svr = sum(rmse_svr_list)/k\n",
    "      avg_rmse_ann = sum(rmse_ann_list)/k\n",
    "      avg_rmse_ridge = sum(rmse_ridge_list)/k\n",
    "      avg_rmse_lasso = sum(rmse_lasso_list)/k\n",
    "\n",
    "      avg_r2_svr = sum(r2_svr_list)/k\n",
    "      avg_r2_ann = sum(r2_ann_list)/k\n",
    "      avg_r2_ridge = sum(r2_ridge_list)/k\n",
    "      avg_r2_lasso = sum(r2_lasso_list)/k\n",
    "\n",
    "\n",
    "      print('Avg MSE, RMSE and r2 score of SVR',avg_mse_svr,avg_rmse_svr,avg_r2_svr)\n",
    "      print('Avg MSE, RMSE and r2 score of ANN',avg_mse_ann,avg_rmse_ann,avg_r2_ann)\n",
    "      print('Avg MSE, RMSE and r2 score of Ridge regression',avg_mse_ridge,avg_rmse_ridge,avg_r2_ridge)\n",
    "      print('Avg MSE, RMSE and r2 score of Lasso regression',avg_mse_lasso,avg_rmse_lasso,avg_r2_lasso)\n",
    "      print(\"\\n\")\n",
    "\n",
    "    def k_fold_classification(self):\n",
    "\n",
    "      k = 5\n",
    "      kf = KFold(n_splits=k, random_state=None)\n",
    "\n",
    "      acc_lr_list = []\n",
    "      acc_svm_list = []\n",
    "      acc_knn_list = []\n",
    "      acc_gau_list = []\n",
    "      acc_rf_list = []\n",
    "      acc_mlp_list = []\n",
    "      acc_nb_list = []\n",
    "\n",
    "      macro_f1_lr_list = []\n",
    "      macro_f1_svm_list = []\n",
    "      macro_f1_knn_list = []\n",
    "      macro_f1_gau_list = []\n",
    "      macro_f1_rf_list = []\n",
    "      macro_f1_mlp_list = []\n",
    "      macro_f1_nb_list = []\n",
    "\n",
    "      \n",
    "      for train_index , test_index in kf.split(self.xtrain):\n",
    "          X_train , X_test = self.xtrain.iloc[train_index,:],self.xtrain.iloc[test_index,:]\n",
    "          y_train , y_test = self.ytrain[train_index],self.ytrain[test_index]\n",
    "          classification_model = classification(X_train,y_train,X_test,y_test)\n",
    "\n",
    "          acc_lr,macro_f1_lr,_ = classification_model.logistic_reg()\n",
    "          acc_svm,macro_f1_svm,_ = classification_model.svm_classification()\n",
    "          acc_knn,macro_f1_knn,_ = classification_model.knn_classification()\n",
    "          acc_gau,macro_f1_gau,_ = classification_model.gaussian_process_classifier()\n",
    "          acc_rf,macro_f1_rf,_ = classification_model.random_forest_classifier()\n",
    "          acc_mlp,macro_f1_mlp,_ = classification_model.mlp_classifier()\n",
    "          acc_nb,macro_f1_nb,_ = classification_model.naives_bayesian()\n",
    "\n",
    "          acc_lr_list.append(acc_lr)\n",
    "          acc_svm_list.append(acc_svm)\n",
    "          acc_knn_list.append(acc_knn)\n",
    "          acc_gau_list.append(acc_gau)\n",
    "          acc_rf_list.append(acc_rf)\n",
    "          acc_mlp_list.append(acc_mlp)\n",
    "          acc_nb_list.append(acc_nb)\n",
    "\n",
    "          macro_f1_lr_list.append(macro_f1_lr)\n",
    "          macro_f1_svm_list.append(macro_f1_svm)\n",
    "          macro_f1_knn_list.append(macro_f1_knn)\n",
    "          macro_f1_gau_list.append(macro_f1_gau)\n",
    "          macro_f1_rf_list.append(macro_f1_rf)\n",
    "          macro_f1_mlp_list.append(macro_f1_mlp)\n",
    "          macro_f1_nb_list.append(macro_f1_nb)\n",
    "\n",
    "      \n",
    "      avg_acc_lr = sum(acc_lr_list)/k\n",
    "      avg_acc_svm = sum(acc_svm_list)/k\n",
    "      avg_acc_knn = sum(acc_knn_list)/k\n",
    "      avg_acc_gau = sum(acc_gau_list)/k\n",
    "      avg_acc_rf = sum(acc_rf_list)/k\n",
    "      avg_acc_mlp = sum(acc_mlp_list)/k\n",
    "      avg_acc_nb = sum(acc_nb_list)/k\n",
    "\n",
    "\n",
    "      avg_macro_f1_lr = sum(macro_f1_lr_list)/k\n",
    "      avg_macro_f1_svm = sum(macro_f1_svm_list)/k\n",
    "      avg_macro_f1_knn = sum(macro_f1_knn_list)/k\n",
    "      avg_macro_f1_gau = sum(macro_f1_gau_list)/k\n",
    "      avg_amacro_f1_rf = sum(macro_f1_rf_list)/k\n",
    "      avg_macro_f1_mlp = sum(macro_f1_mlp_list)/k\n",
    "      avg_macro_f1_nb = sum(macro_f1_nb_list)/k\n",
    "\n",
    "\n",
    "      print('Avg accuracy and macro f1 score of Logistic regression',avg_acc_lr,avg_macro_f1_lr)\n",
    "      print('Avg accuracy and macro f1 score of SVM',avg_acc_svm,avg_macro_f1_svm)\n",
    "      print('Avg accuracy and macro f1 score of KNN',avg_acc_knn,avg_macro_f1_knn)\n",
    "      print('Avg accuracy and macro f1 score of Gaussian Process classifier',avg_acc_gau,avg_macro_f1_gau)\n",
    "      print('Avg accuracy and macro f1 score of Random Forest',avg_acc_rf,avg_amacro_f1_rf)\n",
    "      print('Avg accuracy and macro f1 score of MLP',avg_acc_mlp,avg_macro_f1_mlp)\n",
    "      print('Avg accuracy and macro f1 score of Naives Bayesian',avg_acc_nb,avg_macro_f1_nb)\n",
    "      print(\"\\n\")\n",
    "\n",
    "    def k_fold_classification_regularize(self):\n",
    "\n",
    "      k = 5\n",
    "      kf = KFold(n_splits=k, random_state=None)\n",
    "\n",
    "      acc_l1_list = []\n",
    "      acc_l2_list = []\n",
    "      acc_svm_list = []\n",
    "      acc_mlp_list = []\n",
    "\n",
    "      macro_f1_l1_list = []\n",
    "      macro_f1_l2_list = []\n",
    "      macro_f1_svm_list = []\n",
    "      macro_f1_mlp_list = []\n",
    "\n",
    "      \n",
    "      for train_index , test_index in kf.split(self.xtrain):\n",
    "          X_train , X_test = self.xtrain.iloc[train_index,:],self.xtrain.iloc[test_index,:]\n",
    "          y_train , y_test = self.ytrain[train_index],self.ytrain[test_index]\n",
    "      \n",
    "          classification_model = classification_regularization(X_train,y_train,X_test,y_test)\n",
    "\n",
    "          acc_l1,macro_f1_l1,_ = classification_model.logistic_reg_l1()\n",
    "          acc_l2,macro_f1_l2,_ = classification_model.logistic_reg_l2()\n",
    "          acc_svm,macro_f1_svm,_ = classification_model.svm_classification()\n",
    "          acc_mlp,macro_f1_mlp,_ = classification_model.mlp_classifier()\n",
    "\n",
    "\n",
    "          acc_l1_list.append(acc_l1)\n",
    "          acc_l2_list.append(acc_l2)\n",
    "          acc_svm_list.append(acc_svm)\n",
    "          acc_mlp_list.append(acc_mlp)\n",
    "\n",
    "          macro_f1_l1_list.append(macro_f1_l1)\n",
    "          macro_f1_l2_list.append(macro_f1_l2)\n",
    "          macro_f1_svm_list.append(macro_f1_svm)\n",
    "          macro_f1_mlp_list.append(macro_f1_mlp)\n",
    "\n",
    "      \n",
    "      avg_acc_l1 = sum(acc_l1_list)/k\n",
    "      avg_acc_l2 = sum(acc_l2_list)/k\n",
    "      avg_acc_svm = sum(acc_svm_list)/k\n",
    "      avg_acc_mlp = sum(acc_mlp_list)/k\n",
    "\n",
    "      avg_macro_f1_l1 = sum(macro_f1_l1_list)/k\n",
    "      avg_macro_f1_l2 = sum(macro_f1_l2_list)/k\n",
    "      avg_macro_f1_svm = sum(macro_f1_svm_list)/k\n",
    "      avg_macro_f1_mlp = sum(macro_f1_mlp_list)/k\n",
    "\n",
    "\n",
    "      print('Avg accuracy and macro f1 score of Logistic regression L1',avg_acc_l1,avg_macro_f1_l1)\n",
    "      print('Avg accuracy and macro f1 score of Logistic regression L2',avg_acc_l2,avg_macro_f1_l2)\n",
    "      print('Avg accuracy and macro f1 score of SVM',avg_acc_svm,avg_macro_f1_svm)\n",
    "      print('Avg accuracy and macro f1 score of MLP',avg_acc_mlp,avg_macro_f1_mlp)\n",
    "      print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X7tmdhsxUomu"
   },
   "outputs": [],
   "source": [
    "def regression_kfold(x_train,y_train,x_test,y_test,flag='regularize'):\n",
    "\n",
    "  feature_selected = feature_selection(x_train,y_train)\n",
    "\n",
    "  fs_random = feature_selected.random_forest()\n",
    "  print(\"The performance of different regression models on features selected using random forest\")\n",
    "  val = k_fold_validation(x_train[fs_random],y_train,x_test[fs_random],y_test)\n",
    "  if flag == 'regularize':\n",
    "    val.k_fold_regression_regularize()\n",
    "  else:\n",
    "    val.k_fold_regression()\n",
    "  print(\"\\n\")\n",
    "\n",
    "  fs_mututal = feature_selected.mutual_info()\n",
    "  print(\"The performance of different regression models on features selected using Mutual Information\")\n",
    "  val = k_fold_validation(x_train[fs_mututal],y_train,x_test[fs_mututal],y_test)\n",
    "  if flag == 'regularize':\n",
    "    val.k_fold_regression_regularize()\n",
    "  else:\n",
    "     val.k_fold_regression()\n",
    "  print(\"\\n\")\n",
    "\n",
    "  fs_chi = feature_selected.chi_squared()\n",
    "  print(\"The performance of different regression models on features selected using Chi squared\")\n",
    "  val = k_fold_validation(x_train[fs_chi],y_train,x_test[fs_chi],y_test)\n",
    "  if flag == 'regularize':\n",
    "    val.k_fold_regression_regularize()\n",
    "  else:\n",
    "    val.k_fold_regression()\n",
    "  print(\"\\n\")\n",
    "\n",
    "  fs_seq = feature_selected.sequential_feature()\n",
    "  print(\"The performance of different regression models on features selected using Sequential Features\")\n",
    "  val = k_fold_validation(x_train[fs_seq],y_train,x_test[fs_seq],y_test)\n",
    "  if flag == 'regularize':\n",
    "    val.k_fold_regression_regularize()\n",
    "  else:\n",
    "    val.k_fold_regression()\n",
    "  print(\"\\n\")\n",
    "\n",
    "\n",
    "  fs_rfe = feature_selected.rfe()\n",
    "  print(\"The performance of different regression models on features selected using RFE\")\n",
    "  val = k_fold_validation(x_train[fs_rfe],y_train,x_test[fs_rfe],y_test)\n",
    "  if flag == 'regularize':\n",
    "    val.k_fold_regression_regularize()\n",
    "  else:\n",
    "    val.k_fold_regression()\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "59r_YVZMDe0-"
   },
   "outputs": [],
   "source": [
    "def classification_kfold(x_train,y_train,x_test,y_test,flag='regularize'):\n",
    "\n",
    "  feature_selected = feature_selection(x_train,y_train,task='classification')\n",
    "\n",
    "  fs_random = feature_selected.random_forest()\n",
    "  print(\"The performance of different regression models on features selected using random forest\")\n",
    "  val = k_fold_validation(x_train[fs_random],y_train,x_test[fs_random],y_test)\n",
    "  if flag == 'regularize':\n",
    "    val.k_fold_classification_regularize()\n",
    "  else:\n",
    "     val.k_fold_classification()\n",
    "  print(\"\\n\")\n",
    "\n",
    "  fs_mututal = feature_selected.mutual_info()\n",
    "  print(\"The performance of different regression models on features selected using Mutual Information\")\n",
    "  val = k_fold_validation(x_train[fs_mututal],y_train,x_test[fs_mututal],y_test)\n",
    "  if flag == 'regularize':\n",
    "    val.k_fold_classification_regularize()\n",
    "  else:\n",
    "     val.k_fold_classification()\n",
    "  print(\"\\n\")\n",
    "\n",
    "  fs_chi = feature_selected.chi_squared()\n",
    "  print(\"The performance of different regression models on features selected using Chi squared\")\n",
    "  val = k_fold_validation(x_train[fs_chi],y_train,x_test[fs_chi],y_test)\n",
    "  if flag == 'regularize':\n",
    "    val.k_fold_classification_regularize()\n",
    "  else:\n",
    "     val.k_fold_classification()\n",
    "  print(\"\\n\")\n",
    "\n",
    "  fs_seq = feature_selected.sequential_feature()\n",
    "  print(\"The performance of different regression models on features selected using Sequential Features\")\n",
    "  val = k_fold_validation(x_train[fs_seq],y_train,x_test[fs_seq],y_test)\n",
    "  if flag == 'regularize':\n",
    "    val.k_fold_classification_regularize()\n",
    "  else:\n",
    "     val.k_fold_classification()\n",
    "  print(\"\\n\")\n",
    "\n",
    "  fs_rfe = feature_selected.rfe()\n",
    "  print(\"The performance of different regression models on features selected using RFE\")\n",
    "  val = k_fold_validation(x_train[fs_rfe],y_train,x_test[fs_rfe],y_test)\n",
    "  if flag == 'regularize':\n",
    "    val.k_fold_classification_regularize()\n",
    "  else:\n",
    "     val.k_fold_classification()\n",
    "  print(\"\\n\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bnoLo0B5ZLLw"
   },
   "outputs": [],
   "source": [
    "class performance_regression:\n",
    "  def __init__(self,train_data_path,test_data_path,grade,mission=None):\n",
    "    self.train_data_path = train_data_path\n",
    "    self.test_data_path = test_data_path\n",
    "    self.grade = grade\n",
    "    self.mission = mission\n",
    "\n",
    "  def data_preprocess(self):\n",
    "    pre_proessing_train = pre_processing(self.train_data_path,mission=self.mission)\n",
    "    pre_proessing_test = pre_processing(self.test_data_path,mission=self.mission)\n",
    "\n",
    "    _,self.y_train = pre_proessing_train.preprocess(self.grade)\n",
    "    self.x_train = pre_proessing_train.one_hot_encode()\n",
    "    self.y_train_class = pre_proessing_train.create_class() \n",
    "    self.y_train_classifier = self.y_train_class\n",
    "    _,self.y_test = pre_proessing_test.preprocess(self.grade)\n",
    "    self.x_test = pre_proessing_test.one_hot_encode()\n",
    "    self.y_test_class = pre_proessing_test.create_class() \n",
    "    self.y_test_classifier = self.y_test_class\n",
    "\n",
    "  def reference_systems(self):\n",
    "    #baseline\n",
    "    print(\"PERFORMANCE OF REFERENCE SYSTEMS\")\n",
    "\n",
    "    baseline_reg = baseline_regression(self.x_train,self.y_train,self.x_test,self.y_test)\n",
    "\n",
    "    #trivial system\n",
    "    rmse_ts,mse_ts,r2_ts = baseline_reg.trivial_system()\n",
    "    print(\"Performance of Trivial Systems\")\n",
    "    print(\"rmse of trivial system \", rmse_ts)\n",
    "    print(\"mse of trivial system \", mse_ts)\n",
    "    print(\"r2 score of trivial system \", r2_ts)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    #linear regressor\n",
    "    rmse_lr,mse_lr,r2_lr = baseline_reg.linear_reg()\n",
    "    print(\"Performance of Linear Regressor\")\n",
    "    print(\"rmse of linear regressor \", rmse_lr)\n",
    "    print(\"mse of linear regressor \", mse_lr)\n",
    "    print(\"r2 score of linear regressor \", r2_lr)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    #one_nn\n",
    "    rmse_1nn,mse_1nn,r2_1nn = baseline_reg.one_nn()\n",
    "    print(\"Performance of 1NN\")\n",
    "    print(\"rmse of 1nn \", rmse_1nn)\n",
    "    print(\"mse of 1nn \", mse_1nn)\n",
    "    print(\"r2 score of 1nn \", r2_1nn)\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "  def regression_systems(self,performance = None):\n",
    "  \n",
    "    #PERFORMANCE OF THE REGRESSION SYSTEMS WITHOUT FEATURE SELECTION\n",
    "    print(\"PERFORMANCE OF THE REGRESSION SYSTEMS WITHOUT FEATURE SELECTION\")\n",
    "    reg_model = regression_analysis(self.x_train,self.y_train,self.x_test,self.y_test)\n",
    "    reg_model.perform_regression()\n",
    "\n",
    "    #PERFORMANCE OF THE REGRESSION SYSTEMS WITH FEATURE SELECTION\n",
    "    print(\"PERFORMANCE OF THE REGRESSION SYSTEMS WITH FEATURE SELECTION\")\n",
    "    regresion_feature_selection(self.x_train,self.y_train,self.x_test,self.y_test,flag = None)\n",
    "\n",
    "    #PERFORMANCE OF THE REGRESSION SYSTEMS WITHOUT FEATURE SELECTION WITH NORMALISATION\n",
    "    print(\"PERFORMANCE OF THE REGRESSION SYSTEMS WITHOUT FEATURE SELECTION WITH NORMALISATION\")\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    self.x_train_normalised = self.x_train.copy()\n",
    "    self.x_test_normalised  = self.x_test.copy()\n",
    "    self.x_train_normalised.iloc[:]= scaler.fit_transform(self.x_train.to_numpy())\n",
    "    self.x_test_normalised.iloc[:]  = scaler.transform(self.x_test.to_numpy())\n",
    "\n",
    "    reg_model = regression_analysis(self.x_train_normalised,self.y_train,self.x_test_normalised,self.y_test)\n",
    "    reg_model.perform_regression()\n",
    "\n",
    "    #PERFORMANCE OF THE REGRESSION SYSTEMS WITH FEATURE SELECTION WITH NORMALISATION\n",
    "    print(\"PERFORMANCE OF THE REGRESSION SYSTEMS WITH FEATURE SELECTION WITH NORMALISATION\")\n",
    "    regresion_feature_selection(self.x_train_normalised,self.y_train,self.x_test_normalised,self.y_test,flag = None,mission = self.mission)\n",
    "\n",
    "\n",
    "    if(performance == 'Normalisation'):\n",
    "      print(\"PERFORMANCE OF THE REGRESSION SYSTEMS WITH AND WITHOUT FEATURE SELECTION WITH NORMALISATION WITH REGULARIZATION\")\n",
    "      #PERFORMANCE OF THE REGRESSION SYSTEMS WITHOUT FEATURE SELECTION WITH NORMALISATION WITH REGULARIZATION\n",
    "      print(\"------------------\")\n",
    "      reg_model = regression_analysis(self.x_train_normalised,self.y_train,self.x_test_normalised,self.y_test)\n",
    "      reg_model.perform_regression_regularization()\n",
    "\n",
    "      #PERFORMANCE OF THE REGRESSION SYSTEMS WITH FEATURE SELECTION WITH NORMALISATION WITH REGULARIZATION\n",
    "      regresion_feature_selection(self.x_train_normalised,self.y_train,self.x_test_normalised,self.y_test,mission = self.mission)\n",
    "\n",
    "      print(\"----------------\")\n",
    "      print(\"PERFORMANCE OF THE REGRESSION SYSTEMS WITH AND WITHOUT FEATURE SELECTION WITH NORMALISATION WITH VALIDATION\")\n",
    "      #PERFORMANCE OF THE REGRESSION SYSTEMS WITHOUT FEATURE SELECTION WITH NORMALISATION WITH VALIDATION\n",
    "      val = k_fold_validation(self.x_train_normalised,self.y_train,self.x_test_normalised,self.y_test)\n",
    "      val.k_fold_regression()\n",
    "\n",
    "      #PERFORMANCE OF THE REGRESSION SYSTEMS WITH FEATURE SELECTION WITH NORMALISATION WITH VALIDATION\n",
    "      regression_kfold(self.x_train_normalised,self.y_train,self.x_test_normalised,self.y_test,flag=None)\n",
    "\n",
    "      print(\"----------------\")\n",
    "      print(\"PERFORMANCE OF THE REGRESSION SYSTEMS WITH AND WITHOUT FEATURE SELECTION WITH NORMALISATION WITH VALIDATION AND REGULARIZATION\")\n",
    "\n",
    "      #PERFORMANCE OF THE REGRESSION SYSTEMS WITHOUT FEATURE SELECTION WITH NORMALISATION WITH VALIDATION WITH REGULARISATION\n",
    "      val = k_fold_validation(self.x_train_normalised,self.y_train,self.x_test_normalised,self.y_test)\n",
    "      val.k_fold_regression_regularize()\n",
    "\n",
    "      #PERFORMANCE OF THE REGRESSION SYSTEMS WITH FEATURE SELECTION WITHOUT NORMALISATION WITH VALIDATION WITH REGULARISATION\n",
    "      regression_kfold(self.x_train_normalised,self.y_train,self.x_test_normalised,self.y_test)\n",
    "\n",
    "    else:\n",
    "      #PERFORMANCE OF THE REGRESSION SYSTEMS WITHOUT FEATURE SELECTION WITH REGULARIZATION\n",
    "      print(\"PERFORMANCE OF THE REGRESSION SYSTEMS WITH AND WITHOUT FEATURE SELECTION WITH REGULARIZATION\")\n",
    "      reg_model = regression_analysis(self.x_train,self.y_train,self.x_test,self.y_test)\n",
    "      reg_model.perform_regression_regularization()\n",
    "\n",
    "      #PERFORMANCE OF THE REGRESSION SYSTEMS WITH FEATURE SELECTION WITH NORMALISATION WITH REGULARIZATION\n",
    "      regresion_feature_selection(self.x_train,self.y_train,self.x_test,self.y_test,mission = self.mission)\n",
    "\n",
    "      print(\"----------------\")\n",
    "      print(\"PERFORMANCE OF THE REGRESSION SYSTEMS WITH AND WITHOUT FEATURE SELECTION WITH VALIDATION\")\n",
    "      #PERFORMANCE OF THE REGRESSION SYSTEMS WITHOUT FEATURE SELECTION  WITH VALIDATION\n",
    "      val = k_fold_validation(self.x_train,self.y_train,self.x_test,self.y_test)\n",
    "      val.k_fold_regression()\n",
    "\n",
    "      #PERFORMANCE OF THE REGRESSION SYSTEMS WITH FEATURE SELECTION  WITH VALIDATION\n",
    "      regression_kfold(self.x_train,self.y_train,self.x_test,self.y_test,flag=None)\n",
    "\n",
    "      print(\"----------------\")\n",
    "      print(\"PERFORMANCE OF THE REGRESSION SYSTEMS WITH AND WITHOUT FEATURE SELECTION WITH VALIDATION WITH REGULARISATION\")\n",
    "\n",
    "      #PERFORMANCE OF THE REGRESSION SYSTEMS WITHOUT FEATURE SELECTION WITH VALIDATION WITH REGULARISATION\n",
    "      val = k_fold_validation(self.x_train,self.y_train,self.x_test,self.y_test)\n",
    "      val.k_fold_regression_regularize()\n",
    "\n",
    "      #PERFORMANCE OF THE REGRESSION SYSTEMS WITH FEATURE SELECTION  WITH VALIDATION WITH REGULARISATION\n",
    "      regression_kfold(self.x_train,self.y_train,self.x_test,self.y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nd87O_ubkYlG"
   },
   "outputs": [],
   "source": [
    "class performance_classification:\n",
    "\n",
    "  def __init__(self,train_data_path,test_data_path,grade,mission=None):\n",
    "    self.train_data_path = train_data_path\n",
    "    self.test_data_path = test_data_path\n",
    "    self.grade = grade\n",
    "    self.mission = mission\n",
    "\n",
    "  def data_preprocess(self):\n",
    "    pre_proessing_train = pre_processing(self.train_data_path,mission = self.mission)\n",
    "    pre_proessing_test = pre_processing(self.test_data_path,mission = self.mission)\n",
    "\n",
    "    _,self.y_train = pre_proessing_train.preprocess(self.grade)\n",
    "    self.x_train = pre_proessing_train.one_hot_encode()\n",
    "    self.y_train_class = pre_proessing_train.create_class() \n",
    "    self.y_train_classifier = self.y_train_class\n",
    "\n",
    "    _,self.y_test = pre_proessing_test.preprocess(self.grade)\n",
    "    self.x_test = pre_proessing_test.one_hot_encode()\n",
    "    self.y_test_class = pre_proessing_test.create_class() \n",
    "    self.y_test_classifier = self.y_test_class\n",
    "\n",
    "\n",
    "  def reference_systems(self):\n",
    "\n",
    "    print(\"PERFORMANCE OF REFERENCE SYSTEMS\")\n",
    "    #baseline\n",
    "    baseline_class = baseline_classification(self.x_train,self.y_train_class,self.x_test,self.y_test_class)\n",
    "\n",
    "    #trivial system\n",
    "    acc_ts,macro_f1_ts,cf_ts = baseline_class.trivial_system()\n",
    "    print(\"Performance of Trivial System\")\n",
    "    print(\"Accuracy of trivial system \", acc_ts)\n",
    "    print(\"Macro F-1 score of trivial system \", macro_f1_ts)\n",
    "    print(\"Confusion Matrix of trivial system \\n\", cf_ts)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    #nearest means\n",
    "    acc_nn,macro_f1_nn,cf_nn = baseline_class.nearest_means()\n",
    "    print(\"Performance of Nearest Means system\")\n",
    "    print(\"Accuracy of Nearest Means System \", acc_nn)\n",
    "    print(\"Macro F-1 score of Nearest Means System \", macro_f1_nn)\n",
    "    print(\"Confusion Matrix of of  Nearest Means regressor \\n\", cf_nn)\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "  def classification_systems(self,performance = None):\n",
    "  \n",
    "    print(\"PERFORMANCE OF THE CLASSIFICATION SYSTEMS WITH AND WITHOUT FEATURE SELECTION\")\n",
    "    #PERFORMANCE OF THE CLASSIFICATION SYSTEMS WITHOUT FEATURE SELECTION\n",
    "\n",
    "    class_model = classification_analysis(self.x_train,self.y_train_class,self.x_test,self.y_test_class)\n",
    "    class_model.perform_classification()\n",
    "\n",
    "    #PERFORMANCE OF THE CLASSIFICATION SYSTEMS WITH FEATURE SELECTION\n",
    "    classification_feature_selection(self.x_train,self.y_train_class,self.x_test,self.y_test_class,flag = None,mission = self.mission)\n",
    "\n",
    "    print(\"PERFORMANCE OF THE CLASSIFICATION SYSTEMS WITH AND WITHOUT FEATURE SELECTION WITH NORMALISATION\")\n",
    "    #PERFORMANCE OF THE CLASSIFICATION SYSTEMS WITHOUT FEATURE SELECTION WITH NORMALISATION\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    self.x_train_normalised = self.x_train.copy()\n",
    "    self.x_test_normalised  = self.x_test.copy()\n",
    "    self.x_train_normalised.iloc[:]= scaler.fit_transform(self.x_train.to_numpy())\n",
    "    self.x_test_normalised.iloc[:]  = scaler.transform(self.x_test.to_numpy())\n",
    "\n",
    "    class_model = classification_analysis(self.x_train_normalised,self.y_train_class,self.x_test_normalised,self.y_test_class)\n",
    "    class_model.perform_classification()\n",
    "\n",
    "    #PERFORMANCE OF THE CLASSIFICATION SYSTEMS WITH FEATURE SELECTION WITH NORMALISATION\n",
    "    classification_feature_selection(self.x_train_normalised,self.y_train_class,self.x_test_normalised,self.y_test_class,flag = None,mission = self.mission)\n",
    "\n",
    "\n",
    "    if(performance == 'Normalisation'):\n",
    "      \n",
    "      print(\"PERFORMANCE OF THE CLASSIFICATION SYSTEMS WITH AND WITHOUT FEATURE SELECTION WITH NORMALISATION WITH REGULARIZATION\")\n",
    "      #PERFORMANCE OF THE CLASSIFICATION SYSTEMS WITHOUT FEATURE SELECTION WITH NORMALISATION WITH REGULARIZATION\n",
    "      class_model = classification_analysis(self.x_train,self.y_train_class,self.x_test,self.y_test_class)\n",
    "      class_model.perform_classification_regularisation()\n",
    "\n",
    "      #PERFORMANCE OF THE CLASSIFICATION SYSTEMS WITH FEATURE SELECTION WITH NORMALISATION WITH REGULARIZATION\n",
    "      classification_feature_selection(self.x_train_normalised,self.y_train_class,self.x_test_normalised,self.y_test_class,mission = self.mission)\n",
    "\n",
    "      \n",
    "      y_train_classifier = np.asarray(self.y_train_classifier)\n",
    "      y_test_classifier = np.asarray(self.y_test_classifier)\n",
    "      \n",
    "      print(\"PERFORMANCE OF THE CLASSIFICATION SYSTEMS WITH AND WITHOUT FEATURE SELECTION WITH NORMALISATION WITH VALIDATION\")\n",
    "      #PERFORMANCE OF THE CLASSIFICATION SYSTEMS WITHOUT FEATURE SELECTION WITH NORMALISATION \n",
    "      val = k_fold_validation(self.x_train_normalised,y_train_classifier,self.x_test_normalised,y_test_classifier)\n",
    "      val.k_fold_classification()\n",
    "\n",
    "      #PERFORMANCE OF THE CLASSIFICATION SYSTEMS WITH FEATURE SELECTION WITH NORMALISATION WITH VALIDATION\n",
    "      classification_kfold(self.x_train_normalised,y_train_classifier,self.x_test_normalised,y_test_classifier,flag=None)\n",
    "\n",
    "      print(\"PERFORMANCE OF THE CLASSIFICATION SYSTEMS WITH AND WITHOUT FEATURE SELECTION WITH NORMALISATION WITH VALIDATION WITH REGULARISATION\")\n",
    "      #PERFORMANCE OF THE CLASSIFICATION SYSTEMS WITHOUT FEATURE SELECTION WITH NORMALISATION WITH VALIDATION WITH REGULARISATION\n",
    "      val = k_fold_validation(self.x_train_normalised,y_train_classifier,self.x_test_normalised,y_test_classifier)\n",
    "      val.k_fold_classification_regularize()\n",
    "\n",
    "      #PERFORMANCE OF THE CLASSIFICATION SYSTEMS WITH FEATURE SELECTION WITHOUT NORMALISATION WITH VALIDATION WITH REGULARISATION\n",
    "      classification_kfold(self.x_train_normalised,y_train_classifier,self.x_test_normalised,y_test_classifier)\n",
    "\n",
    "    else:\n",
    "      #PERFORMANCE OF THE CLASSIFICATION SYSTEMS WITHOUT FEATURE SELECTION WITH REGULARIZATION\n",
    "      print(\"---------------------\")\n",
    "      print(\"PERFORMANCE OF THE CLASSIFICATION SYSTEMS WITH AND WITHOUT FEATURE SELECTION WITH REGULARIZATION\")\n",
    "      class_model = classification_analysis(self.x_train,self.y_train_class,self.x_test,self.y_test_class)\n",
    "      class_model.perform_classification_regularisation()\n",
    "\n",
    "      #PERFORMANCE OF THE CLASSIFICATION SYSTEMS WITH FEATURE SELECTION WITH NORMALISATION WITH REGULARIZATION\n",
    "      classification_feature_selection(self.x_train,self.y_train_class,self.x_test,self.y_test_class,mission = self.mission)\n",
    "\n",
    "      y_train_classifier = np.asarray(self.y_train_classifier)\n",
    "      y_test_classifier = np.asarray(self.y_test_classifier)\n",
    "\n",
    "      #PERFORMANCE OF THE CLASSIFICATION SYSTEMS WITHOUT FEATURE SELECTION  WITH VALIDATION\n",
    "      print(\"---------------------\")\n",
    "      print(\"PERFORMANCE OF THE CLASSIFICATION SYSTEMS WITH AND WITHOUT FEATURE SELECTION  WITH VALIDATION\")\n",
    "      val = k_fold_validation(self.x_train,y_train_classifier,self.x_test,y_test_classifier)\n",
    "      val.k_fold_classification()\n",
    "\n",
    "      #PERFORMANCE OF THE CLASSIFICATION SYSTEMS WITH FEATURE SELECTION  WITH VALIDATION\n",
    "      classification_kfold(self.x_train,y_train_classifier,self.x_test,self.y_test_classifier,flag=None)\n",
    "\n",
    "      #PERFORMANCE OF THE CLASSIFICATION SYSTEMS WITHOUT FEATURE SELECTION WITH VALIDATION WITH REGULARISATION\n",
    "      print(\"---------------------\")\n",
    "      print(\"PERFORMANCE OF THE CLASSIFICATION SYSTEMS WITH AND WITHOUT FEATURE SELECTION WITH VALIDATION WITH REGULARISATION\")\n",
    "      val = k_fold_validation(self.x_train,y_train_classifier,self.x_test,y_test_classifier)\n",
    "      val.k_fold_classification_regularize()\n",
    "\n",
    "      #PERFORMANCE OF THE CLASSIFICATION SYSTEMS WITH FEATURE SELECTION  WITH VALIDATION WITH REGULARISATION\n",
    "      classification_kfold(self.x_train,y_train_classifier,self.x_test,y_test_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jd61uAEYi1Bg"
   },
   "source": [
    "MISSION-1 REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJaaFTS0hsAk"
   },
   "outputs": [],
   "source": [
    "performance_reg = performance_regression('student_performance_train.csv','student_performance_test.csv','G1')\n",
    "performance_reg.data_preprocess()\n",
    "performance_reg.reference_systems()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"The performance of regression systems\")\n",
    "performance_reg.regression_systems(performance = 'Normalisation')\n",
    "print(\"----------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U23nNSrZl75r"
   },
   "source": [
    "MISSION-1 CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UtrvJolcmFTO"
   },
   "outputs": [],
   "source": [
    "performance_class = performance_classification('student_performance_train.csv','student_performance_test.csv','G1')\n",
    "performance_class.data_preprocess()\n",
    "performance_class.reference_systems()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"The performance of classification systems\")\n",
    "performance_class.classification_systems()\n",
    "print(\"----------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3Z2hcSDmGMw"
   },
   "source": [
    "MISSION-2 REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O6notnur3vlj"
   },
   "outputs": [],
   "source": [
    "performance_reg = performance_regression('student_performance_train.csv','student_performance_test.csv','G3')\n",
    "performance_reg.data_preprocess()\n",
    "performance_reg.reference_systems()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"The performance of regression systems\")\n",
    "performance_reg.regression_systems()\n",
    "print(\"----------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Ju47LabmPLx"
   },
   "source": [
    "MISSION-2 CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kasW_gWYmOrP"
   },
   "outputs": [],
   "source": [
    "performance_class = performance_classification('student_performance_train.csv','student_performance_test.csv','G3')\n",
    "performance_class.data_preprocess()\n",
    "print(\"The performance of reference systems\")\n",
    "performance_class.reference_systems()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"The performance of regression systems\")\n",
    "performance_class.classification_systems(performance = 'Normalisation')\n",
    "print(\"----------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfsRr8c0mU5Z"
   },
   "source": [
    "MISSION-3 REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G921AFL5DnFZ"
   },
   "outputs": [],
   "source": [
    "performance_reg = performance_regression('student_performance_train.csv','student_performance_test.csv','G3',mission = '3')\n",
    "performance_reg.data_preprocess()\n",
    "performance_reg.reference_systems()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"The performance of regression systems\")\n",
    "performance_reg.regression_systems()\n",
    "print(\"----------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DFbZRRhma2Z"
   },
   "source": [
    "MISSION-3 CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tTahwD43DjRq"
   },
   "outputs": [],
   "source": [
    "performance_class = performance_classification('student_performance_train.csv','student_performance_test.csv','G3',mission = '3')\n",
    "performance_class.data_preprocess()\n",
    "print(\"The performance of reference systems\")\n",
    "performance_class.reference_systems()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"The performance of regression systems\")\n",
    "performance_class.classification_systems()\n",
    "print(\"----------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqd2IKrZad7a"
   },
   "source": [
    "Hyper parameter tuning for mission 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pIGy4euPRVnS"
   },
   "outputs": [],
   "source": [
    "pre_proessing_train_m1r = pre_processing('student_performance_train.csv')\n",
    "pre_proessing_test_m1r = pre_processing('student_performance_test.csv')\n",
    "\n",
    "_,y_train_m1r = pre_proessing_train_m1r.preprocess('G1')\n",
    "x_train_m1r = pre_proessing_train_m1r.one_hot_encode()\n",
    "y_train_class_m1r = pre_proessing_train_m1r.create_class() \n",
    "y_train_classifier_m1r = y_train_class_m1r\n",
    "\n",
    "_,y_test_m1r = pre_proessing_test_m1r.preprocess('G1')\n",
    "x_test_m1r = pre_proessing_test_m1r.one_hot_encode()\n",
    "y_test_class_m1r = pre_proessing_test_m1r.create_class() \n",
    "y_test_classifier_m1r = y_test_class_m1r\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "x_train_normalised_m1r = x_train_m1r.copy()\n",
    "x_test_normalised_m1r  = x_test_m1r.copy()\n",
    "x_train_normalised_m1r.iloc[:]= scaler.fit_transform(x_train_m1r.to_numpy())\n",
    "x_test_normalised_m1r.iloc[:]  = scaler.transform(x_test_m1r.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-I24nQ58bIhW"
   },
   "outputs": [],
   "source": [
    "#mission1 regression - gradient boost + RFE \n",
    "\n",
    "rmse_list = []\n",
    "r2_list = []\n",
    "for i in range(1,51):\n",
    "  rfe_selector = RFE(estimator=LinearRegression(),n_features_to_select = i, step = 2)\n",
    "  rfe_selector.fit(x_train_normalised_m1r,y_train_m1r)\n",
    "  col_req_list = x_train_normalised_m1r.columns[rfe_selector.get_support()]\n",
    "\n",
    "  reg = ensemble.GradientBoostingRegressor().fit(x_train_normalised_m1r[col_req_list], y_train_m1r)\n",
    "  y_pred = reg.predict(x_test_normalised_m1r[col_req_list])\n",
    "  mse = mean_squared_error(y_test_m1r, y_pred)\n",
    "  rmse =  mean_squared_error(y_test_m1r, y_pred, squared=False)\n",
    "  rmse_list.append(rmse)\n",
    "  r2 = r2_score(y_test_m1r, y_pred) \n",
    "  r2_list.append(r2)\n",
    "\n",
    "plt.figure(0)\n",
    "plt.plot(rmse_list,label = 'rmse list')\n",
    "plt.plot(r2_list, label = 'r2 list')\n",
    "plt.legend()\n",
    "min_rmse_index_rfe = rmse_list.index(min(rmse_list)) \n",
    "max_r2_index_rfe = r2_list.index(max(r2_list)) \n",
    "\n",
    "\n",
    "rfe_selector = RFE(estimator=LinearRegression(),n_features_to_select = min_rmse_index_rfe, step = 2)\n",
    "rfe_selector.fit(x_train_normalised_m1r,y_train_m1r)\n",
    "col_req_list = x_train_normalised_m1r.columns[rfe_selector.get_support()]\n",
    "rmse_list = []\n",
    "r2_list = []\n",
    "\n",
    "for i in range(1,201):\n",
    "  reg = ensemble.GradientBoostingRegressor(n_estimators = i).fit(x_train_normalised_m1r[col_req_list], y_train_m1r)\n",
    "  y_pred = reg.predict(x_test_normalised_m1r[col_req_list])\n",
    "  mse = mean_squared_error(y_test_m1r, y_pred)\n",
    "  rmse =  mean_squared_error(y_test_m1r, y_pred, squared=False)\n",
    "  rmse_list.append(rmse)\n",
    "  r2 = r2_score(y_test_m1r, y_pred) \n",
    "  r2_list.append(r2)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(rmse_list,label = 'rmse list')\n",
    "plt.plot(r2_list, label = 'r2 list')\n",
    "plt.legend()\n",
    "min_rmse_index_gb = rmse_list.index(min(rmse_list)) \n",
    "max_r2_index_gb = r2_list.index(max(r2_list)) \n",
    "\n",
    "rfe_selector = RFE(estimator=LinearRegression(),n_features_to_select = min_rmse_index_rfe, step = 2)\n",
    "rfe_selector.fit(x_train_normalised_m1r,y_train_m1r)\n",
    "col_req_list = x_train_normalised_m1r.columns[rfe_selector.get_support()]\n",
    "\n",
    "\n",
    "reg = ensemble.GradientBoostingRegressor(n_estimators = min_rmse_index_gb).fit(x_train_normalised_m1r[col_req_list], y_train_m1r)\n",
    "y_pred = reg.predict(x_test_normalised_m1r[col_req_list])\n",
    "mse = mean_squared_error(y_test_m1r, y_pred)\n",
    "rmse =  mean_squared_error(y_test_m1r, y_pred, squared=False)\n",
    "r2 = r2_score(y_test_m1r, y_pred) \n",
    "\n",
    "print(rmse)\n",
    "print(r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r46Rz8-ELPqP"
   },
   "outputs": [],
   "source": [
    "pre_proessing_train_m2r = pre_processing('student_performance_train.csv')\n",
    "pre_proessing_test_m2r = pre_processing('student_performance_test.csv')\n",
    "\n",
    "_,y_train_m2r = pre_proessing_train_m2r.preprocess('G3')\n",
    "x_train_m2r = pre_proessing_train_m2r.one_hot_encode()\n",
    "y_train_class_m2r = pre_proessing_train_m2r.create_class() \n",
    "y_train_classifier_m2r = y_train_class_m2r\n",
    "\n",
    "_,y_test_m2r = pre_proessing_test_m2r.preprocess('G3')\n",
    "x_test_m2r = pre_proessing_test_m2r.one_hot_encode()\n",
    "y_test_class_m2r = pre_proessing_test_m2r.create_class() \n",
    "y_test_classifier_m2r = y_test_class_m2r\n",
    "\n",
    "scaler_m2r = preprocessing.MinMaxScaler()\n",
    "x_train_normalised_m2r = x_train_m2r.copy()\n",
    "x_test_normalised_m2r  = x_test_m2r.copy()\n",
    "x_train_normalised_m2r.iloc[:]= scaler_m2r.fit_transform(x_train_m2r.to_numpy())\n",
    "x_test_normalised_m2r.iloc[:]  = scaler_m2r.transform(x_test_m2r.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VgsTni6ZDwf5"
   },
   "outputs": [],
   "source": [
    "#mission 2 regression\n",
    "\n",
    "rmse_list = []\n",
    "r2_list = []\n",
    "for i in range(1,26):\n",
    "  selector = SelectKBest(chi2, k = i)\n",
    "  selector.fit(x_train_m2r, y_train_m2r)\n",
    "  selected_feat = x_train_m2r.columns[selector.get_support()]\n",
    "\n",
    "  reg = ensemble.GradientBoostingRegressor().fit(x_train_m2r[selected_feat], y_train_m2r)\n",
    "  y_pred = reg.predict(x_test_m2r[selected_feat])\n",
    "  mse = mean_squared_error(y_test_m2r, y_pred)\n",
    "  rmse =  mean_squared_error(y_test_m2r, y_pred, squared=False)\n",
    "  rmse_list.append(rmse)\n",
    "  r2 = r2_score(y_test_m2r, y_pred) \n",
    "  r2_list.append(r2)\n",
    "\n",
    "plt.figure(0)\n",
    "plt.plot(rmse_list,label = 'rmse list')\n",
    "plt.plot(r2_list, label = 'r2 list')\n",
    "plt.legend()\n",
    "min_rmse_index_rfe = rmse_list.index(min(rmse_list)) \n",
    "max_r2_index_rfe = r2_list.index(max(r2_list)) \n",
    "\n",
    "print(min_rmse_index_rfe)\n",
    "print(max_r2_index_rfe)\n",
    "\n",
    "\n",
    "selector = SelectKBest(chi2, k = min_rmse_index_rfe)\n",
    "selector.fit(x_train_m2r, y_train_m2r)\n",
    "selected_feat = x_train_m2r.columns[selector.get_support()]\n",
    "rmse_list = []\n",
    "r2_list = []\n",
    "\n",
    "for i in range(1,201):\n",
    "  reg = ensemble.GradientBoostingRegressor(n_estimators = i).fit(x_train_m2r[selected_feat], y_train_m2r)\n",
    "  y_pred = reg.predict(x_test_m2r[selected_feat])\n",
    "  mse = mean_squared_error(y_test_m2r, y_pred)\n",
    "  rmse =  mean_squared_error(y_test_m2r, y_pred, squared=False)\n",
    "  rmse_list.append(rmse)\n",
    "  r2 = r2_score(y_test_m2r, y_pred) \n",
    "  r2_list.append(r2)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(rmse_list,label = 'rmse list')\n",
    "plt.plot(r2_list, label = 'r2 list')\n",
    "plt.legend()\n",
    "min_rmse_index_gb = rmse_list.index(min(rmse_list)) \n",
    "max_r2_index_gb = r2_list.index(max(r2_list)) \n",
    "\n",
    "print(min_rmse_index_gb)\n",
    "print(max_r2_index_gb)\n",
    "\n",
    "\n",
    "selector = SelectKBest(chi2, k = min_rmse_index_rfe)\n",
    "selector.fit(x_train_m2r, y_train_m2r)\n",
    "selected_feat = x_train_m2r.columns[selector.get_support()]\n",
    "\n",
    "\n",
    "reg = ensemble.GradientBoostingRegressor().fit(x_train_m2r[selected_feat], y_train_m2r)\n",
    "y_pred = reg.predict(x_test_m2r[selected_feat])\n",
    "mse = mean_squared_error(y_test_m2r, y_pred)\n",
    "rmse =  mean_squared_error(y_test_m2r, y_pred, squared=False)\n",
    "r2 = r2_score(y_test_m2r, y_pred) \n",
    "\n",
    "print(rmse)\n",
    "print(r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a5ws6TIZNAaN"
   },
   "outputs": [],
   "source": [
    "proessing_train = pre_processing('student_performance_train.csv',mission='3')\n",
    "pre_proessing_test = pre_processing('student_performance_test.csv',mission='3')\n",
    "\n",
    "_,y_train = proessing_train.preprocess('G3')\n",
    "x_train = proessing_train.one_hot_encode()\n",
    "y_train_class = proessing_train.create_class() \n",
    "y_train_classifier = y_train_class\n",
    "\n",
    "_,y_test = pre_proessing_test.preprocess('G3')\n",
    "x_test = pre_proessing_test.one_hot_encode()\n",
    "y_test_class = pre_proessing_test.create_class() \n",
    "y_test_classifier = y_test_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gqq8daFVFzQz"
   },
   "outputs": [],
   "source": [
    "#mission3 regression - ANN + mutual info\n",
    "\n",
    "\n",
    "rmse_list = []\n",
    "r2_list = []\n",
    "for i in range(1,26):\n",
    "  selector = SelectKBest(mutual_info_regression, k = i)\n",
    "  selector.fit(x_train, y_train)\n",
    "  selected_feat = x_train.columns[selector.get_support()]\n",
    "\n",
    "  regr = MLPRegressor(random_state=1, max_iter=500).fit(x_train[selected_feat], y_train)\n",
    "  y_pred = regr.predict(x_test[selected_feat])\n",
    "  mse = mean_squared_error(y_test, y_pred)\n",
    "  rmse =  mean_squared_error(y_test, y_pred, squared=False)\n",
    "  r2 = r2_score(y_test, y_pred)   \n",
    "  rmse_list.append(rmse)\n",
    "  r2 = r2_score(y_test, y_pred) \n",
    "  r2_list.append(r2)\n",
    "\n",
    "plt.figure(0)\n",
    "plt.plot(rmse_list,label = 'rmse list')\n",
    "plt.plot(r2_list, label = 'r2 list')\n",
    "plt.legend()\n",
    "min_rmse_index_rfe = rmse_list.index(min(rmse_list)) \n",
    "max_r2_index_rfe = r2_list.index(max(r2_list)) \n",
    "\n",
    "print(min_rmse_index_rfe)\n",
    "print(max_r2_index_rfe)\n",
    "print(min(rmse_list))\n",
    "print(max(r2_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSDYVv8XflML"
   },
   "source": [
    "PLOTTING GRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZyeE2SDgryK"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fs_list = ['all_feat','random_for ','mutual_info ','chi_squared ','seq_feature','RFE']\n",
    "\n",
    "#SVR\n",
    " #-w/o norm, with norm , with reg+norm,\n",
    " # -w/o norm, with norm and val, with reg+norm, with reg+val+norm\n",
    "\n",
    "data_svr = [[2.46,2.64,2.43,2.40,2.42,2.48],\n",
    "[2.45,2.64,2.55,2.55,2.42,2.46],\n",
    "[2.33,2.52,2.36,2.29,2.27,2.27]]\n",
    "\n",
    "\n",
    "data_ann = [[2.50,2.74,2.47,2.46,2.37,2.38],\n",
    "[2.64,2.64,2.53,2.53,2.37,2.40],\n",
    "[2.47,2.57,2.35,2.35,2.28,2.31]]\n",
    "\n",
    "\n",
    "data_gb = [[2.30,2.74,2.45,2.35,2.38,2.37],\n",
    "[2.31,2.74,2.44,2.36,2.38,2.35],\n",
    "[2.37,2.54,2.42,2.35,2.32,2.36]]\n",
    "\n",
    "data_knn = [[2.70,2.72,2.61,2.64,2.57,2.63],\n",
    "[2.63,2.46,2.57,2.57,2.55,2.55],\n",
    "[2.42,2.51,2.40,2.41,2.32,2.33]]\n",
    "\n",
    "X = np.arange(6)\n",
    "fig, axs = plt.subplots(2, 2,figsize=(16,16))\n",
    "fig.suptitle('MISSION-I Regression',size=16)\n",
    "plt.setp(axs,xticks = range(len(fs_list)), xticklabels= fs_list)\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set( ylabel='RMSE')\n",
    "    ax.tick_params(axis='x', labelrotation=45)\n",
    "\n",
    "\n",
    "axs[0,0].bar(X + 0.00, data_svr[0], color = 'c', width = 0.25, label = 'without norm')\n",
    "axs[0,0].bar(X + 0.25, data_svr[1], color = '#497ce3', width = 0.25,label = 'with norm' )\n",
    "axs[0,0].bar(X + 0.50, data_svr[2], color = '#8649e3', width = 0.25, label = 'with norm and cross-validation')\n",
    "axs[0,0].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[0,0].set_title('SVR')\n",
    "\n",
    "axs[0,1].bar(X + 0.00, data_ann[0], color = 'c', width = 0.25, label = 'without norm')\n",
    "axs[0,1].bar(X + 0.25, data_ann[1], color = '#497ce3', width = 0.25,label = 'with norm' )\n",
    "axs[0,1].bar(X + 0.50, data_ann[2], color = '#8649e3', width = 0.25, label = 'with norm and cross-validation')\n",
    "axs[0,1].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[0,1].set_title('ANN')\n",
    "\n",
    "axs[1,0].bar(X + 0.00, data_gb[0], color = 'c', width = 0.25, label = 'without norm')\n",
    "axs[1,0].bar(X + 0.25, data_gb[1], color = '#497ce3', width = 0.25,label = 'with norm' )\n",
    "axs[1,0].bar(X + 0.50, data_gb[2], color = '#8649e3', width = 0.25, label = 'with norm and cross-validation')\n",
    "axs[1,0].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[1,0].set_title('Gradient Boost')\n",
    "\n",
    "axs[1,1].bar(X + 0.00, data_knn[0], color = 'c', width = 0.25, label = 'without norm')\n",
    "axs[1,1].bar(X + 0.25, data_knn[1], color = '#497ce3', width = 0.25,label = 'with norm' )\n",
    "axs[1,1].bar(X + 0.50, data_knn[2], color = '#8649e3', width = 0.25, label = 'with norm and cross-validation')\n",
    "axs[1,1].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[1,1].set_title('KNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87UxRaLKFCbG"
   },
   "outputs": [],
   "source": [
    "fs_list = ['all_features','random forest','mutual info','chi_squared','seq_feature','RFE']\n",
    "\n",
    "#SVR\n",
    " #-w/o norm, with norm , with reg+norm,\n",
    " # -w/o norm, with norm and val, with reg+norm, with reg+val+norm\n",
    "\n",
    "data_svr = [[2.45,2.63,2.50,2.50,2.42,2.46],\n",
    "[2.35,2.53,2.30,2.30,2.28,2.28]]\n",
    "\n",
    "data_ann = [[2.58,2.63,2.46,2.46,2.37,2.39],\n",
    "[2.47,2.57,2.31,2.35,2.28,2.33]]\n",
    "\n",
    "data_ridge = [[2.42,2.63,2.45,2.45,2.40,2.42],\n",
    "[2.32,2.52,2.27,2.27,2.26,2.26]]\n",
    "\n",
    "data_lasso = [[2.55,2.73,2.55,2.55,2.55,2.55],\n",
    "[2.37,2.57,2.37,2.37,2.37,2.3]]\n",
    "\n",
    "\n",
    "X = np.arange(6)\n",
    "fig, axs = plt.subplots(2, 2,figsize=(15,15))\n",
    "fig.suptitle('MISSION-I Regression')\n",
    "plt.setp(axs,xticks = range(len(fs_list)), xticklabels= fs_list)\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set( ylabel='RMSE')\n",
    "    ax.tick_params(axis='x', labelrotation=45)\n",
    "\n",
    "axs[0,0].bar(X + 0.00, data_svr[0], color = 'c', width = 0.25, label = 'with regularization')\n",
    "axs[0,0].bar(X + 0.25, data_svr[1], color = '#497ce3', width = 0.25,label = 'with regularization and cross-validation' )\n",
    "axs[0,0].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[0,0].set_title('SVR')\n",
    "\n",
    "axs[0,1].bar(X + 0.00, data_ann[0], color = 'c', width = 0.25, label = 'with regularization')\n",
    "axs[0,1].bar(X + 0.25, data_ann[1], color = '#497ce3', width = 0.25,label = 'with regularization and cross-validation' )\n",
    "axs[0,1].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[0,1].set_title('ANN')\n",
    "\n",
    "axs[1,0].bar(X + 0.00, data_ridge[0], color = 'c', width = 0.25, label = 'with regularization')\n",
    "axs[1,0].bar(X + 0.25, data_ridge[1], color = '#497ce3', width = 0.25,label = 'with regularization and cross-validation' )\n",
    "axs[1,0].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[1,0].set_title('Ridge')\n",
    "\n",
    "axs[1,1].bar(X + 0.00, data_lasso[0], color = 'c', width = 0.25, label = 'with regularization')\n",
    "axs[1,1].bar(X + 0.25, data_lasso[1], color = '#497ce3', width = 0.25,label = 'with regularization and cross-validation')\n",
    "axs[1,1].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[1,1].set_title('Lasso')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_H0opryJ6Sf5"
   },
   "outputs": [],
   "source": [
    "#mission 1 - classifcation\n",
    "\n",
    "data_lr = [[0.30,0.32,0.27,0.30,0.30,0.34],\n",
    "[0.30,0.32,0.24,0.30,0.30,0.34],\n",
    "[0.27,0.27,0.26,0.27,0.32,0.39]]\n",
    "\n",
    "data_svm = [[0.28,0.27,0.27,0.31,0.32,0.32],\n",
    "[0.28,0.27,0.27,0.31,0.32,0.32],\n",
    "[0.22,0.26,0.27,0.25,0.30,0.30]]\n",
    "\n",
    "data_gau = [[0.18,0.30,0.18,0.17,0.26,0.27],\n",
    "[0.19,0.31,0.21,0.17,0.26,0.27],\n",
    "[0.17,0.08,0.19,0.08,0.20,0.20]]\n",
    "\n",
    "data_rf = [[0.22,0.22,0.27,0.24,0.23,0.21],\n",
    "[0.22,0.22,0.25,0.24,0.23,0.22],\n",
    "[0.24,0.22,0.22,0.24,0.24,0.22]]\n",
    "\n",
    "data_mlp = [[0.29,0.32,0.30,0.31,0.35,0.34],\n",
    "[0.29,0.32,0.29,0.31,0.35,0.34],\n",
    "[0.27,0.29,0.26,0.27,0.35,0.30]]\n",
    "\n",
    "data_nb = [[0.21,0.17,0.16,0.21,0.22,0.20],\n",
    "[0.21,0.18,0.20,0.21,0.22,0.21],\n",
    "[0.19,0.16,0.17,0.19,0.20,0.18]]\n",
    "\n",
    "data_knn = [[0.24,0.25,0.28,0.25,0.30,0.28],\n",
    "[0.24,0.25,0.23,0.25,0.32,0.28],\n",
    "[0.22,0.21,0.27,0.23,0.28,0.25]]\n",
    "\n",
    "X = np.arange(6)\n",
    "fig, axs = plt.subplots(4, 2,figsize=(19,19))\n",
    "fig.suptitle('MISSION-I Classification')\n",
    "plt.setp(axs,xticks = range(len(fs_list)), xticklabels= fs_list)\n",
    "axs[3,1].set_axis_off()\n",
    "for ax in axs.flat:\n",
    "    ax.set( ylabel='Macro F-1 score')\n",
    "    #ax.tick_params(axis='x', labelrotation=30)\n",
    "\n",
    "axs[0,0].bar(X + 0.00, data_lr[0], color = 'c', width = 0.25, label = 'without norm')\n",
    "axs[0,0].bar(X + 0.25, data_lr[1], color = '#497ce3', width = 0.25,label = 'with norm' )\n",
    "axs[0,0].bar(X + 0.50, data_lr[2], color = '#8649e3', width = 0.25, label = 'without norm and cross-validation')\n",
    "axs[0,0].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[0,0].set_title('Logistic Regression')\n",
    "\n",
    "axs[0,1].bar(X + 0.00, data_svm[0], color = 'c', width = 0.25, label = 'without norm')\n",
    "axs[0,1].bar(X + 0.25, data_svm[1], color = '#497ce3', width = 0.25,label = 'with norm' )\n",
    "axs[0,1].bar(X + 0.50, data_svm[2], color = '#8649e3', width = 0.25, label = 'without norm and cross-validation')\n",
    "axs[0,1].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[0,1].set_title('SVM')\n",
    "\n",
    "axs[1,0].bar(X + 0.00, data_gau[0], color = 'c', width = 0.25, label = 'without norm')\n",
    "axs[1,0].bar(X + 0.25, data_gau[1], color = '#497ce3', width = 0.25,label = 'with norm' )\n",
    "axs[1,0].bar(X + 0.50, data_gau[2], color = '#8649e3', width = 0.25, label = 'without norm and cross-validation')\n",
    "axs[1,0].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[1,0].set_title('Gaussian')\n",
    "\n",
    "axs[1,1].bar(X + 0.00, data_rf[0], color = 'c', width = 0.25, label = 'without norm')\n",
    "axs[1,1].bar(X + 0.25, data_rf[1], color = '#497ce3', width = 0.25,label = 'with norm' )\n",
    "axs[1,1].bar(X + 0.50, data_rf[2], color = '#8649e3', width = 0.25, label = 'without norm and cross-validation')\n",
    "axs[1,1].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[1,1].set_title('Random Forest')\n",
    "\n",
    "axs[2,0].bar(X + 0.00, data_mlp[0], color = 'c', width = 0.25, label = 'without norm')\n",
    "axs[2,0].bar(X + 0.25, data_mlp[1], color = '#497ce3', width = 0.25,label = 'with norm' )\n",
    "axs[2,0].bar(X + 0.50, data_mlp[2], color = '#8649e3', width = 0.25, label = 'without norm and cross-validation')\n",
    "axs[2,0].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[2,0].set_title('MLP')\n",
    "\n",
    "axs[2,1].bar(X + 0.00, data_nb[0], color = 'c', width = 0.25, label = 'without norm')\n",
    "axs[2,1].bar(X + 0.25, data_nb[1], color = '#497ce3', width = 0.25,label = 'with norm' )\n",
    "axs[2,1].bar(X + 0.50, data_nb[2], color = '#8649e3', width = 0.25, label = 'without norm and cross-validation')\n",
    "axs[2,1].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[2,1].set_title('Naive bayes')\n",
    "\n",
    "axs[3,0].bar(X + 0.00, data_knn[0], color = 'c', width = 0.25, label = 'without norm')\n",
    "axs[3,0].bar(X + 0.25, data_knn[1], color = '#497ce3', width = 0.25,label = 'with norm' )\n",
    "axs[3,0].bar(X + 0.50, data_knn[2], color = '#8649e3', width = 0.25, label = 'without norm and cross-validation')\n",
    "axs[3,0].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[3,0].set_title('KNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "atvrQVdd8gWA"
   },
   "outputs": [],
   "source": [
    "#reg and val\n",
    "\n",
    "#logistic l1 \n",
    "data_l1 = [[0.30,0.31,0.32,0.33,0.32,0.31],\n",
    "[0.28,0.26,0.25,0.28,0.30,0.38]]\n",
    "\n",
    "data_l2 = [[0.29,0.30,0.30,0.31,0.32,0.31],\n",
    "[0.27,0.26,0.25,0.27,0.32,0.30]]\n",
    "\n",
    "data_svm = [[0.29,0.29,0.30,0.30,0.37,0.31,0.30],\n",
    "[0.27,0.28,0.27,0.26,0.30,0.32,0]]\n",
    "\n",
    "data_mlp = [[0.27,0.26,0.30,0.30,0.37,0.30,0.30],\n",
    "[0.26,0.25,0.26,0.28,0.32,0.30,0]]\n",
    "\n",
    "X = np.arange(6)\n",
    "fig, axs = plt.subplots(2, 2,figsize=(15,15))\n",
    "fig.suptitle('MISSION-I Classification')\n",
    "plt.setp(axs,xticks = range(len(fs_list)), xticklabels= fs_list)\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set( ylabel='Macro F-1 score')\n",
    "    ax.tick_params(axis='x', labelrotation=45)    \n",
    "\n",
    "axs[0,0].bar(X + 0.00, data_svr[0], color = 'c', width = 0.25, label = 'with regularization')\n",
    "axs[0,0].bar(X + 0.25, data_svr[1], color = '#497ce3', width = 0.25,label = 'with regularization and cross-validation' )\n",
    "axs[0,0].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[0,0].set_title('Logistic regression L1')\n",
    "\n",
    "axs[0,1].bar(X + 0.00, data_ann[0], color = 'c', width = 0.25, label = 'with regularization')\n",
    "axs[0,1].bar(X + 0.25, data_ann[1], color = '#497ce3', width = 0.25,label = 'with regularization and cross-validation' )\n",
    "axs[0,1].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[0,1].set_title('Logistic regression L2')\n",
    "\n",
    "axs[1,0].bar(X + 0.00, data_ridge[0], color = 'c', width = 0.25, label = 'with regularization')\n",
    "axs[1,0].bar(X + 0.25, data_ridge[1], color = '#497ce3', width = 0.25,label = 'with regularization and cross-validation' )\n",
    "axs[1,0].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[1,0].set_title('SVM')\n",
    "\n",
    "axs[1,1].bar(X + 0.00, data_lasso[0], color = 'c', width = 0.25, label = 'with regularization')\n",
    "axs[1,1].bar(X + 0.25, data_lasso[1], color = '#497ce3', width = 0.25,label = 'with regularization and cross-validation')\n",
    "axs[1,1].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[1,1].set_title('MLP')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jWWG-7xcsJIU"
   },
   "outputs": [],
   "source": [
    "#mission 2 \n",
    "\n",
    "#SVM -w/o norm, with norm,val\n",
    "data_svr = [[2.72,2.96,2.80,2.73,2.81,2.78],\n",
    "[2.70,2.96,2.74,2.74,2.80,2.72],\n",
    "[2.78,3.00,2.80,2.73,2.71,2.76]]\n",
    "\n",
    "data_ann = [[2.83,3.09,2.80,2.80,2.76,2.67],\n",
    "[2.85,3.00,2.69,2.70,2.86,2.71],\n",
    "[2.80,3.10,2.81,2.80,2.73,2.78]]\n",
    "\n",
    "data_gb = [[2.64,3.15,2.84,2.62,2.82,2.83],\n",
    "[2.64,3.15,2.78,2.68,2.82,2.87],\n",
    "[2.87,3.04,2.92,2.86,2.77,2.84]]\n",
    "\n",
    "data_knn = [[3,3.04,3.01,2.97,2.97,2.73],\n",
    "[2.89,3.03,2.81,2.77,2.86,2.84],\n",
    "[2.91,2.98,3.01,2.97,2.95,2.85]]\n",
    "\n",
    "\n",
    "X = np.arange(6)\n",
    "fig, axs = plt.subplots(2, 2,figsize=(15,15))\n",
    "fig.suptitle('MISSION-II Regression')\n",
    "plt.setp(axs,xticks = range(len(fs_list)), xticklabels= fs_list)\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set( ylabel='RMSE')\n",
    "    ax.tick_params(axis='x', labelrotation=45)\n",
    "\n",
    "axs[0,0].bar(X + 0.00, data_svr[0], color = 'c', width = 0.25, label = 'without norm')\n",
    "axs[0,0].bar(X + 0.25, data_svr[1], color = '#497ce3', width = 0.25,label = 'with norm' )\n",
    "axs[0,0].bar(X + 0.50, data_svr[2], color = '#8649e3', width = 0.25, label = 'without norm and cross-validation')\n",
    "axs[0,0].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[0,0].set_title('SVR')\n",
    "\n",
    "axs[0,1].bar(X + 0.00, data_ann[0], color = 'c', width = 0.25, label = 'without norm')\n",
    "axs[0,1].bar(X + 0.25, data_ann[1], color = '#497ce3', width = 0.25,label = 'with norm' )\n",
    "axs[0,1].bar(X + 0.50, data_ann[2], color = '#8649e3', width = 0.25, label = 'without norm and cross-validation')\n",
    "axs[0,1].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[0,1].set_title('ANN')\n",
    "\n",
    "axs[1,0].bar(X + 0.00, data_gb[0], color = 'c', width = 0.25, label = 'without norm')\n",
    "axs[1,0].bar(X + 0.25, data_gb[1], color = '#497ce3', width = 0.25,label = 'with norm' )\n",
    "axs[1,0].bar(X + 0.50, data_gb[2], color = '#8649e3', width = 0.25, label = 'without norm and cross-validation')\n",
    "axs[1,0].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[1,0].set_title('Gradient Boost')\n",
    "\n",
    "axs[1,1].bar(X + 0.00, data_knn[0], color = 'c', width = 0.25, label = 'without norm')\n",
    "axs[1,1].bar(X + 0.25, data_knn[1], color = '#497ce3', width = 0.25,label = 'with norm' )\n",
    "axs[1,1].bar(X + 0.50, data_knn[2], color = '#8649e3', width = 0.25, label = 'without norm and cross-validation')\n",
    "axs[1,1].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[1,1].set_title('KNN')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "05BYUcGT0cKT"
   },
   "outputs": [],
   "source": [
    "#reg and val\n",
    "\n",
    "#svr\n",
    "data_svr = [[2.71,2.96,2.77,2.77,2.81,2.70],\n",
    "[2.78,3.00,2.82,2.73,2.71,2.76]]\n",
    "\n",
    "data_ann = [[2.90,3.00,2.75,2.76,2.78,2.71],\n",
    "[2.81,3.08,2.77,2.79,2.73,2.76]]\n",
    "\n",
    "data_ridge = [[2.71,3.00,2.75,2.75,2.79,2.70],\n",
    "[2.78,3.02,2.80,2.74,2.69,2.74]]\n",
    "\n",
    "data_lasso = [[2.79,3.05,2.80,2.80,2.80,2.80],\n",
    "[2.78,3.02,2.81,2.77,2.75,2.76]]\n",
    "\n",
    "\n",
    "X = np.arange(6)\n",
    "fig, axs = plt.subplots(2, 2,figsize=(15,15))\n",
    "fig.suptitle('MISSION-II Regression')\n",
    "plt.setp(axs,xticks = range(len(fs_list)), xticklabels= fs_list)\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set( ylabel='RMSE')\n",
    "    ax.tick_params(axis='x', labelrotation=45)\n",
    "\n",
    "axs[0,0].bar(X + 0.00, data_svr[0], color = 'c', width = 0.25, label = 'with regularization')\n",
    "axs[0,0].bar(X + 0.25, data_svr[1], color = '#497ce3', width = 0.25,label = 'with regularization and cross-validation' )\n",
    "axs[0,0].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[0,0].set_title('SVR')\n",
    "\n",
    "axs[0,1].bar(X + 0.00, data_ann[0], color = 'c', width = 0.25, label = 'with regularization')\n",
    "axs[0,1].bar(X + 0.25, data_ann[1], color = '#497ce3', width = 0.25,label = 'with regularization and cross-validation' )\n",
    "axs[0,1].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[0,1].set_title('ANN')\n",
    "\n",
    "axs[1,0].bar(X + 0.00, data_ridge[0], color = 'c', width = 0.25, label = 'with regularization')\n",
    "axs[1,0].bar(X + 0.25, data_ridge[1], color = '#497ce3', width = 0.25,label = 'with regularization and cross-validation' )\n",
    "axs[1,0].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[1,0].set_title('Ridge')\n",
    "\n",
    "axs[1,1].bar(X + 0.00, data_lasso[0], color = 'c', width = 0.25, label = 'with regularization')\n",
    "axs[1,1].bar(X + 0.25, data_lasso[1], color = '#497ce3', width = 0.25,label = 'with regularization and cross-validation')\n",
    "axs[1,1].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[1,1].set_title('Lasso')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O3-ld0lwCRM6"
   },
   "outputs": [],
   "source": [
    "#logisitc reg -w/o norm, with norm, with validation \n",
    "data_lr = [[0.31,0.18,0.28,0.30,0.26,0.26],\n",
    "[0.30,0.19,0.22,0.32,0.28,0.26],\n",
    "[0.30,0.27,0.29,0.31,0.33,0.33]]\n",
    "\n",
    "data_svm = [[0.31,0.09,0.28,0.28,0.20,0.26],\n",
    "[0.25,0.25,0.23,0.25,0.27,0.28],\n",
    "[0.27,0.26,0.30,0.31,0.32,0.35]]\n",
    "\n",
    "data_gau = [[0.14,0.09,0.20,0.09,0.18,0.20],\n",
    "[0.21,0.20,0.23,0.24,0.27,0.26],\n",
    "[0.26,0.25,0.21,0.27,0.29,0.31]]\n",
    "\n",
    "data_rf = [[0.16,0.13,0.16,0.16,0.19,0.16],\n",
    "[0.16,0.14,0.17,0.16,0.19,0.15],\n",
    "[0.20,0.26,0.23,0.22,0.29,0.27]]\n",
    "\n",
    "data_mlp = [[0.31,0.25,0.30,0.32,0.29,0.30],\n",
    "[0.37,0.27,0.32,0.31,0.27,0.30],\n",
    "[0.15,0.16,0.21,0.16,0.22,0.17]]\n",
    "\n",
    "data_nb = [[0.27,0.20,0.29,0.26,0.27,0.21],\n",
    "[0.25,0.17,0.25,0.22,0.24,0.27],\n",
    "[0.32,0.32,0.32,0.34,0.34,0.36]]\n",
    "\n",
    "data_knn = [[0.20,0.17,0.18,0.18,0.16,0.19],\n",
    "[0.22,0.27,0.22,0.25,0.29,0.17],\n",
    "[0.25,0.20,0.23,0.25,0.23,0.26]]\n",
    "\n",
    "X = np.arange(6)\n",
    "fig, axs = plt.subplots(4, 2,figsize=(17,17))\n",
    "fig.suptitle('MISSION-II Classification')\n",
    "plt.setp(axs,xticks = range(len(fs_list)), xticklabels= fs_list)\n",
    "axs[3,1].set_axis_off()\n",
    "for ax in axs.flat:\n",
    "    ax.set( ylabel='Macro F-1 Score')\n",
    "\n",
    "\n",
    "axs[0,0].bar(X + 0.00, data_lr[0], color = 'c', width = 0.25, label = 'without norm')\n",
    "axs[0,0].bar(X + 0.25, data_lr[1], color = '#497ce3', width = 0.25,label = 'with norm' )\n",
    "axs[0,0].bar(X + 0.50, data_lr[2], color = '#8649e3', width = 0.25, label = 'with norm and cross-validation')\n",
    "axs[0,0].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[0,0].set_title('Logistic Regression')\n",
    "\n",
    "axs[0,1].bar(X + 0.00, data_svm[0], color = 'c', width = 0.25, label = 'without norm')\n",
    "axs[0,1].bar(X + 0.25, data_svm[1], color = '#497ce3', width = 0.25,label = 'with norm' )\n",
    "axs[0,1].bar(X + 0.50, data_svm[2], color = '#8649e3', width = 0.25, label = 'with norm and cross-validation')\n",
    "axs[0,1].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[0,1].set_title('SVM')\n",
    "\n",
    "axs[1,0].bar(X + 0.00, data_gau[0], color = 'c', width = 0.25, label = 'without norm')\n",
    "axs[1,0].bar(X + 0.25, data_gau[1], color = '#497ce3', width = 0.25,label = 'with norm' )\n",
    "axs[1,0].bar(X + 0.50, data_gau[2], color = '#8649e3', width = 0.25, label = 'with norm and cross-validation')\n",
    "axs[1,0].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[1,0].set_title('Gaussian')\n",
    "\n",
    "axs[1,1].bar(X + 0.00, data_rf[0], color = 'c', width = 0.25, label = 'without norm')\n",
    "axs[1,1].bar(X + 0.25, data_rf[1], color = '#497ce3', width = 0.25,label = 'with norm' )\n",
    "axs[1,1].bar(X + 0.50, data_rf[2], color = '#8649e3', width = 0.25, label = 'with norm and cross-validation')\n",
    "axs[1,1].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[1,1].set_title('Random Forest')\n",
    "\n",
    "axs[2,0].bar(X + 0.00, data_mlp[0], color = 'c', width = 0.25, label = 'without norm')\n",
    "axs[2,0].bar(X + 0.25, data_mlp[1], color = '#497ce3', width = 0.25,label = 'with norm' )\n",
    "axs[2,0].bar(X + 0.50, data_mlp[2], color = '#8649e3', width = 0.25, label = 'with norm and cross-validation')\n",
    "axs[2,0].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[2,0].set_title('MLP')\n",
    "\n",
    "axs[2,1].bar(X + 0.00, data_nb[0], color = 'c', width = 0.25, label = 'without norm')\n",
    "axs[2,1].bar(X + 0.25, data_nb[1], color = '#497ce3', width = 0.25,label = 'with norm' )\n",
    "axs[2,1].bar(X + 0.50, data_nb[2], color = '#8649e3', width = 0.25, label = 'with norm and cross-validation')\n",
    "axs[2,1].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[2,1].set_title('Naive Bayes')\n",
    "\n",
    "axs[3,0].bar(X + 0.00, data_knn[0], color = 'c', width = 0.25, label = 'without norm')\n",
    "axs[3,0].bar(X + 0.25, data_knn[1], color = '#497ce3', width = 0.25,label = 'with norm' )\n",
    "axs[3,0].bar(X + 0.50, data_knn[2], color = '#8649e3', width = 0.25, label = 'with norm and cross-validation')\n",
    "axs[3,0].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[3,0].set_title('KNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OF1S07xM5NIA"
   },
   "outputs": [],
   "source": [
    "#reg and val\n",
    "\n",
    "#logisitic reg - l1\n",
    "data_l1 = [[0.25,0.21,0.18,0.27,0.24,0.26],\n",
    "[0.31,0.22,0.24,0.28,0.33,0.32]]\n",
    "\n",
    "data_l2 = [[0.31,0.19,0.21,0.32,0.28,0.26],\n",
    "[0.30,0.23,0.25,0.31,0.33,0.33]]\n",
    "\n",
    "data_svm = [[0.29,0.20,0.28,0.34,0.29,0.27],\n",
    "[0.30,0.28,0.32,0.31,0.38,0.39]]\n",
    "\n",
    "data_mlp = [[0.30,0.25,0.31,0.33,0.28,0.31],\n",
    "[0.33,0.29,0.28,0.33,0.34,0.36]]\n",
    "\n",
    "\n",
    "\n",
    "X = np.arange(6)\n",
    "fig, axs = plt.subplots(2, 2,figsize=(15,15))\n",
    "fig.suptitle('MISSION-II Classification')\n",
    "plt.setp(axs,xticks = range(len(fs_list)), xticklabels= fs_list)\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set( ylabel='Macro F-1 score')\n",
    "    ax.tick_params(axis='x', labelrotation=45)\n",
    "\n",
    "axs[0,0].bar(X + 0.00, data_l1[0], color = 'c', width = 0.25, label = 'with regularization')\n",
    "axs[0,0].bar(X + 0.25, data_l1[1], color = '#497ce3', width = 0.25,label = 'with regularization and cross-validation' )\n",
    "axs[0,0].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[0,0].set_title('Logisitc Regression L1')\n",
    "\n",
    "axs[0,1].bar(X + 0.00, data_l2[0], color = 'c', width = 0.25, label = 'with regularization')\n",
    "axs[0,1].bar(X + 0.25, data_l2[1], color = '#497ce3', width = 0.25,label = 'with regularization and cross-validation' )\n",
    "axs[0,1].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[0,1].set_title('Logisitc Regression L2')\n",
    "\n",
    "axs[1,0].bar(X + 0.00, data_svm[0], color = 'c', width = 0.25, label = 'with regularization')\n",
    "axs[1,0].bar(X + 0.25, data_svm[1], color = '#497ce3', width = 0.25,label = 'with regularization and cross-validation' )\n",
    "axs[1,0].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[1,0].set_title('SVM')\n",
    "\n",
    "\n",
    "axs[1,1].bar(X + 0.00, data_mlp[0], color = 'c', width = 0.25, label = 'with regularization')\n",
    "axs[1,1].bar(X + 0.25, data_mlp[1], color = '#497ce3', width = 0.25,label = 'with regularization and cross-validation' )\n",
    "axs[1,1].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[1,1].set_title('MLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p5BD7MVv0-f7"
   },
   "outputs": [],
   "source": [
    "#mission 3\n",
    "\n",
    "\n",
    "#SVM -w/o norm, with norm,val + w/o\n",
    "data_svr = [[0.99,0.97,0.96,0.98,0.96,0.95],\n",
    "[1.07,1.05,1.05,1.08,1.04,1.09],\n",
    "[1.35,1.35,1.34,1.33,1.34,1.34]]\n",
    "\n",
    "data_ann = [[1.27,1.00,0.94,1.05,1.00,1.04],\n",
    "[1.10,1.08,1.07,1.02,1.03,1.12],\n",
    "[1.33,1.32,1.32,1.42,1.29,1.32]]\n",
    "\n",
    "data_gb = [[1.19,1.18,1.06,1.13,1.23,1.31],\n",
    "[1.11,1.18,1.06,1.28,1.23,1.22],\n",
    "[1.41,1.36,1.51,1.36,1.29,1.37]]\n",
    "\n",
    "data_knn = [[1.13,1.13,1.16,1.10,1.15,1.13],\n",
    "[2.60,2.12,1.87,2.37,1.97,2.17],\n",
    "[1.48,1.50,1.44,1.51,1.37,1.40]]\n",
    "\n",
    "\n",
    "\n",
    "X = np.arange(6)\n",
    "fig, axs = plt.subplots(2, 2,figsize=(15,15))\n",
    "fig.suptitle('MISSION-III Regression')\n",
    "plt.setp(axs,xticks = range(len(fs_list)), xticklabels= fs_list)\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set( ylabel='RMSE')\n",
    "    ax.tick_params(axis='x', labelrotation=45)\n",
    "\n",
    "axs[0,0].bar(X + 0.00, data_svr[0], color = 'c', width = 0.25, label = 'without norm')\n",
    "axs[0,0].bar(X + 0.25, data_svr[1], color = '#497ce3', width = 0.25,label = 'with norm' )\n",
    "axs[0,0].bar(X + 0.50, data_svr[2], color = '#8649e3', width = 0.25, label = 'without norm and cross-validation')\n",
    "axs[0,0].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[0,0].set_title('SVR')\n",
    "\n",
    "axs[0,1].bar(X + 0.00, data_ann[0], color = 'c', width = 0.25, label = 'without norm')\n",
    "axs[0,1].bar(X + 0.25, data_ann[1], color = '#497ce3', width = 0.25,label = 'with norm' )\n",
    "axs[0,1].bar(X + 0.50, data_ann[2], color = '#8649e3', width = 0.25, label = 'without norm and cross-validation')\n",
    "axs[0,1].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[0,1].set_title('ANN')\n",
    "\n",
    "axs[1,0].bar(X + 0.00, data_gb[0], color = 'c', width = 0.25, label = 'without norm')\n",
    "axs[1,0].bar(X + 0.25, data_gb[1], color = '#497ce3', width = 0.25,label = 'with norm' )\n",
    "axs[1,0].bar(X + 0.50, data_gb[2], color = '#8649e3', width = 0.25, label = 'without norm and cross-validation')\n",
    "axs[1,0].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[1,0].set_title('Gradient Boost')\n",
    "\n",
    "axs[1,1].bar(X + 0.00, data_knn[0], color = 'c', width = 0.25, label = 'without norm')\n",
    "axs[1,1].bar(X + 0.25, data_knn[1], color = '#497ce3', width = 0.25,label = 'with norm' )\n",
    "axs[1,1].bar(X + 0.50, data_knn[2], color = '#8649e3', width = 0.25, label = 'without norm and cross-validation')\n",
    "axs[1,1].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[1,1].set_title('KNN')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YjPWl9lH8XvV"
   },
   "outputs": [],
   "source": [
    "#reg and val\n",
    "\n",
    "#svr\n",
    "\n",
    "\n",
    "data_svr = [[0.99,0.97,0.97,0.97,0.96,0.95],\n",
    "[1.93,1.93,1.93,1.87,1.91,1.92]]\n",
    "\n",
    "data_ann = [[1.22,1.02,1.00,1.00,0.99,1.10],\n",
    "[2.0,1.85,1.81,2.06,1.75,1.82]]\n",
    "\n",
    "data_ridge = [[1.01,0.96,0.96,0.96,0.96,0.98],\n",
    "[1.90,1.96,1.95,1.87,1.77,1.98]]\n",
    "\n",
    "data_lasso = [[0.86,0.86,0.86,0.86,0.86,0.86],\n",
    "[1.32,2.37,2.46,2.41,1.94,2.06]]\n",
    "\n",
    "\n",
    "X = np.arange(6)\n",
    "fig, axs = plt.subplots(2, 2,figsize=(15,15))\n",
    "fig.suptitle('MISSION-III Regression')\n",
    "plt.setp(axs,xticks = range(len(fs_list)), xticklabels= fs_list)\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set( ylabel='RMSE')\n",
    "    ax.tick_params(axis='x', labelrotation=45)\n",
    "\n",
    "axs[0,0].bar(X + 0.00, data_svr[0], color = 'c', width = 0.25, label = 'with regularization')\n",
    "axs[0,0].bar(X + 0.25, data_svr[1], color = '#497ce3', width = 0.25,label = 'with regularization and cross-validation' )\n",
    "axs[0,0].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[0,0].set_title('SVR')\n",
    "\n",
    "axs[0,1].bar(X + 0.00, data_ann[0], color = 'c', width = 0.25, label = 'with regularization')\n",
    "axs[0,1].bar(X + 0.25, data_ann[1], color = '#497ce3', width = 0.25,label = 'with regularization and cross-validation' )\n",
    "axs[0,1].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[0,1].set_title('ANN')\n",
    "\n",
    "axs[1,0].bar(X + 0.00, data_ridge[0], color = 'c', width = 0.25, label = 'with regularization')\n",
    "axs[1,0].bar(X + 0.25, data_ridge[1], color = '#497ce3', width = 0.25,label = 'with regularization and cross-validation' )\n",
    "axs[1,0].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[1,0].set_title('Ridge')\n",
    "\n",
    "axs[1,1].bar(X + 0.00, data_lasso[0], color = 'c', width = 0.25, label = 'with regularization')\n",
    "axs[1,1].bar(X + 0.25, data_lasso[1], color = '#497ce3', width = 0.25,label = 'with regularization and cross-validation')\n",
    "axs[1,1].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[1,1].set_title('Lasso')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I685NuRC-9xw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#logisitc reg -w/o norm, with norm, with validation \n",
    "data_lr = [[0.66,0.60,0.62,0.61,0.64,0.62],\n",
    "[0.57,0.61,0.60,0.64,0.54,0.52],\n",
    "[0.60,0.57,0.60,0.55,0.58,0.59]]\n",
    "\n",
    "#SVM\n",
    "data_svm = [[0.74,0.76,0.77,0.76,0.76,0.76],\n",
    "[0.65,0.72,0.73,0.69,0.74,0.72],\n",
    "[0.70,0.75,0.71,0.73,0.74,0.73]]\n",
    "\n",
    "#guassian\n",
    "data_gau = [[0.70,0.76,0.70,0.09,0.65,0.75],\n",
    "[0.32,0.73,0.65,0.58,0.64,0.55],\n",
    "[0.70,0.68,0.67,0.09,0.42,0.67]]\n",
    "\n",
    "#Random forest\n",
    "data_rf = [[0.46,0.68,0.70,0.61,0.70,0.46],\n",
    "[0.46,0.68,0.72,0.65,0.70,0.49],\n",
    "[0.47,0.66,0.65,0.58,0.64,0.48]]\n",
    "\n",
    "#MLP\n",
    "data_mlp = [[0.60,0.63,0.65,0.57,0.66,0.66],\n",
    "[0.58,0.72,0.70,0.67,0.68,0.65],\n",
    "[0.58,0.60,0.55,0.60,0.60,0.60]]\n",
    "\n",
    "#naive bayes\n",
    "data_nb = [[0.55,0.72,0.48,0.53,0.48,0.54],\n",
    "[0.50,0.72,0.51,0.53,0.44,0.52],\n",
    "[0.52,0.71,0.50,0.53,0.48,0.51]]\n",
    "\n",
    "#knn\n",
    "data_knn = [[0.66,0.74,0.74,0.68,0.67,0.73],\n",
    "[0.28,0.58,0.51,0.32,0.42,0.28],\n",
    "[0.66,0.69,0.70,0.65,0.69,0.72]]\n",
    "\n",
    "\n",
    "\n",
    "X = np.arange(6)\n",
    "fig, axs = plt.subplots(4, 2,figsize=(17,17))\n",
    "fig.suptitle('MISSION-III Classification')\n",
    "plt.setp(axs,xticks = range(len(fs_list)), xticklabels= fs_list)\n",
    "axs[3,1].set_axis_off()\n",
    "for ax in axs.flat:\n",
    "    ax.set( ylabel='Macro F-1 score')\n",
    "    #x.tick_params(axis='x', labelrotation=45)\n",
    "\n",
    "axs[0,0].bar(X + 0.00, data_lr[0], color = 'c', width = 0.25, label = 'without norm')\n",
    "axs[0,0].bar(X + 0.25, data_lr[1], color = '#497ce3', width = 0.25,label = 'with norm' )\n",
    "axs[0,0].bar(X + 0.50, data_lr[2], color = '#8649e3', width = 0.25, label = 'without norm and cross-validation')\n",
    "axs[0,0].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[0,0].set_title('Logistic Regression')\n",
    "\n",
    "axs[0,1].bar(X + 0.00, data_svm[0], color = 'c', width = 0.25, label = 'without norm')\n",
    "axs[0,1].bar(X + 0.25, data_svm[1], color = '#497ce3', width = 0.25,label = 'with norm' )\n",
    "axs[0,1].bar(X + 0.50, data_svm[2], color = '#8649e3', width = 0.25, label = 'without norm and cross-validation')\n",
    "axs[0,1].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[0,1].set_title('SVM')\n",
    "\n",
    "axs[1,0].bar(X + 0.00, data_gau[0], color = 'c', width = 0.25, label = 'without norm')\n",
    "axs[1,0].bar(X + 0.25, data_gau[1], color = '#497ce3', width = 0.25,label = 'with norm' )\n",
    "axs[1,0].bar(X + 0.50, data_gau[2], color = '#8649e3', width = 0.25, label = 'without norm and cross-validation')\n",
    "axs[1,0].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[1,0].set_title('Gaussian')\n",
    "\n",
    "axs[1,1].bar(X + 0.00, data_rf[0], color = 'c', width = 0.25, label = 'without norm')\n",
    "axs[1,1].bar(X + 0.25, data_rf[1], color = '#497ce3', width = 0.25,label = 'with norm' )\n",
    "axs[1,1].bar(X + 0.50, data_rf[2], color = '#8649e3', width = 0.25, label = 'without norm and cross-validation')\n",
    "axs[1,1].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[1,1].set_title('Random Forest')\n",
    "\n",
    "axs[2,0].bar(X + 0.00, data_mlp[0], color = 'c', width = 0.25, label = 'without norm')\n",
    "axs[2,0].bar(X + 0.25, data_mlp[1], color = '#497ce3', width = 0.25,label = 'with norm' )\n",
    "axs[2,0].bar(X + 0.50, data_mlp[2], color = '#8649e3', width = 0.25, label = 'without norm and cross-validation')\n",
    "axs[2,0].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[2,0].set_title('MLP')\n",
    "\n",
    "axs[2,1].bar(X + 0.00, data_nb[0], color = 'c', width = 0.25, label = 'without norm')\n",
    "axs[2,1].bar(X + 0.25, data_nb[1], color = '#497ce3', width = 0.25,label = 'with norm' )\n",
    "axs[2,1].bar(X + 0.50, data_nb[2], color = '#8649e3', width = 0.25, label = 'without norm and cross-validation')\n",
    "axs[2,1].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[2,1].set_title('Naive Bayes')\n",
    "\n",
    "axs[3,0].bar(X + 0.00, data_knn[0], color = 'c', width = 0.25, label = 'without norm')\n",
    "axs[3,0].bar(X + 0.25, data_knn[1], color = '#497ce3', width = 0.25,label = 'with norm' )\n",
    "axs[3,0].bar(X + 0.50, data_knn[2], color = '#8649e3', width = 0.25, label = 'without norm and cross-validation')\n",
    "axs[3,0].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[3,0].set_title('KNN')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mrHfGSD_PTy5"
   },
   "outputs": [],
   "source": [
    "#reg and val\n",
    "\n",
    "#logistic l1 \n",
    "data_l1 = [[0.58,0.61,0.59,0.62,0.64,0.61],\n",
    "[0.57,0.60,0.58,0.59,0.60,0.55]]\n",
    "\n",
    "#logistic l2\n",
    "data_l2 = [[0.66,0.62,0.63,0.61,0.64,0.62],\n",
    "[0.60,0.58,0.62,0.55,0.58,0.58]]\n",
    "\n",
    "#SVM\n",
    "data_svm = [[0.72,0.74,0.70,0.75,0.76,0.75],\n",
    "[0.72,0.74,0.75,0.71,0.73,0.71]]\n",
    "\n",
    "#MLP\n",
    "data_mlp = [[0.60,0.61,0.61,0.57,0.63,0.65],\n",
    "[0.58,0.60,0.60,0.60,0.60,0.59]]\n",
    "\n",
    "\n",
    "X = np.arange(6)\n",
    "fig, axs = plt.subplots(2, 2,figsize=(15,15))\n",
    "fig.suptitle('MISSION-III Classification')\n",
    "plt.setp(axs,xticks = range(len(fs_list)), xticklabels= fs_list)\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set( ylabel='Macro F-1 score')\n",
    "    ax.tick_params(axis='x', labelrotation=45)\n",
    "\n",
    "axs[0,0].bar(X + 0.00, data_l1[0], color = 'c', width = 0.25, label = 'with regularization')\n",
    "axs[0,0].bar(X + 0.25, data_l1[1], color = '#497ce3', width = 0.25,label = 'with regularization and cross-validation' )\n",
    "axs[0,0].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[0,0].set_title('Logistic Regression L1')\n",
    "\n",
    "axs[0,1].bar(X + 0.00, data_l2[0], color = 'c', width = 0.25, label = 'with regularization')\n",
    "axs[0,1].bar(X + 0.25, data_l2[1], color = '#497ce3', width = 0.25,label = 'with regularization and cross-validation' )\n",
    "axs[0,1].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[0,1].set_title('Logistic Regression L2')\n",
    "\n",
    "axs[1,0].bar(X + 0.00, data_svm[0], color = 'c', width = 0.25, label = 'with regularization')\n",
    "axs[1,0].bar(X + 0.25, data_svm[1], color = '#497ce3', width = 0.25,label = 'with regularization and cross-validation' )\n",
    "axs[1,0].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[1,0].set_title('SVM')\n",
    "\n",
    "\n",
    "axs[1,1].bar(X + 0.00, data_mlp[0], color = 'c', width = 0.25, label = 'with regularization')\n",
    "axs[1,1].bar(X + 0.25, data_mlp[1], color = '#497ce3', width = 0.25,label = 'with regularization and cross-validation' )\n",
    "axs[1,1].legend(loc=\"upper right\",prop={'size': 7})\n",
    "axs[1,1].set_title('MLP')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "EE559_final_project code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
